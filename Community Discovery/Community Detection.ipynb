{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pickle\n",
    "import copy\n",
    "from cdlib import algorithms\n",
    "import os\n",
    "import time\n",
    "from cdlib import algorithms\n",
    "\n",
    "with open('./SECCG generated Knowledge Graph.pkl', 'rb') as f:\n",
    "    G_Value = pickle.load(f)\n",
    "undirected_G_Value=copy.deepcopy(G_Value)\n",
    "G_Value_undirected=undirected_G_Value.to_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import pickle\n",
    "from copy import copy\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from math import log, e\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "def filter_graph_by_behave_conf(graph):\n",
    "    # Create a new graph with edges that have behave_conf > 0.5\n",
    "    new_graph = nx.Graph()\n",
    "    for edge in graph.edges:\n",
    "        if graph[edge[0]][edge[1]].get(\"behave_conf\", 0) > 0.5:\n",
    "            new_graph.add_edge(edge[0], edge[1], **graph[edge[0]][edge[1]])\n",
    "\n",
    "    # Remove isolated nodes from the new graph\n",
    "    new_graph.remove_nodes_from(list(nx.isolates(new_graph)))\n",
    "    return new_graph\n",
    "\n",
    "def get_graph_tactics(graph):\n",
    "    tactic_list = []\n",
    "    for edge in graph.edges:\n",
    "        one_hot = graph[edge[0]][edge[1]].get(\"tactic_conf\")\n",
    "        tactic_this = [i for i, x in enumerate(one_hot) if x == 1]\n",
    "        tactic_list.append(tactic_this)\n",
    "    return tactic_list\n",
    "\n",
    "def check_unique_tactic(tactic_list):\n",
    "    total_tactic_list = set()\n",
    "    for single_tactic_list in tactic_list:\n",
    "        for tactic in single_tactic_list:\n",
    "            total_tactic_list.add(tactic)\n",
    "    return len(total_tactic_list)\n",
    "\n",
    "def check_graph_by_unique_article_id(graph):\n",
    "    id_set = set()\n",
    "    for edge in graph.edges:\n",
    "        if graph[edge[0]][edge[1]].get(\"article_id\") is not None:\n",
    "            id_set.add(graph[edge[0]][edge[1]][\"article_id\"])\n",
    "    # check size of the set >=2\n",
    "    if len(id_set) >= 2:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_graph_has_tactic(graph):\n",
    "    # Create a new graph with edges that have behave_conf > 0.5\n",
    "    for edge in graph.edges:\n",
    "        if graph[edge[0]][edge[1]].get(\"tactic_conf\") is not None:\n",
    "            if \"1\" in str(graph[edge[0]][edge[1]][\"tactic_conf\"]):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def new_draw_graph(\n",
    "    graph, draw_edges=True, save_folder=\"defaultgraph\", saveorshow=\"show\"\n",
    "):\n",
    "    pos = nx.circular_layout(graph) \n",
    "    plt.figure(num=None, figsize=(20, 20), dpi=100)\n",
    "    nx.draw_networkx(\n",
    "        graph,\n",
    "        pos,\n",
    "        with_labels=True,\n",
    "        node_size=20,\n",
    "        arrowsize=90,\n",
    "        linewidths=1.5,\n",
    "        arrowstyle=\"->\",\n",
    "        edge_color=\"red\",\n",
    "        node_shape=\"o\",\n",
    "        bbox=dict(facecolor=\"black\", edgecolor=\"black\", boxstyle=\"round,pad=0.3\"),\n",
    "        node_color=\"black\",\n",
    "        font_size=15,\n",
    "        font_color=\"white\",\n",
    "        # labels=nx.get_node_attributes(graph, 'label'),\n",
    "    )\n",
    "    if draw_edges:\n",
    "        edge_labels = nx.get_edge_attributes(graph, \"relation\")\n",
    "        articles_labels = nx.get_edge_attributes(graph, \"article_id\")\n",
    "        one_hot = nx.get_edge_attributes(graph, \"tactic_conf\")\n",
    "        true_tactic = {}\n",
    "        for key in one_hot.keys():\n",
    "            true_tactic[key] = [i for i, x in enumerate(one_hot[key]) if x == 1]\n",
    "        # tactic_this=[i for i, x in enumerate(one_hot) if x == 1]\n",
    "        data = articles_labels\n",
    "        article_dict = {}\n",
    "        article_id = 0\n",
    "        for key, value in data.items():\n",
    "            if value not in article_dict:\n",
    "                article_dict[value] = \"from article\" + str(article_id)\n",
    "                article_id = article_id + 1\n",
    "        result_dict = {}\n",
    "        for key, value in data.items():\n",
    "            result_dict[value] = article_dict[value]\n",
    "        for key, value in edge_labels.items():\n",
    "            new_value = \"relation:\" + value\n",
    "            article_true_value = articles_labels[key]\n",
    "            article_012_value = result_dict[article_true_value]\n",
    "            new_value += f\"\\n{article_012_value}\"\n",
    "            if key in true_tactic:\n",
    "                if len(true_tactic[key]) > 0:\n",
    "                    new_value = new_value + \"\\nattack tactic:\\n\"\n",
    "                    for tactic in true_tactic[key]:\n",
    "                        new_value += f\"{big_label_list[tactic]},\\n\"\n",
    "            edge_labels[key] = new_value\n",
    "        nx.draw_networkx_edge_labels(\n",
    "            graph,\n",
    "            pos,\n",
    "            edge_labels=edge_labels,\n",
    "            font_color=\"red\",\n",
    "            font_size=12,\n",
    "        )\n",
    "    if saveorshow == \"show\":\n",
    "        plt.show()\n",
    "    if saveorshow == \"save\":\n",
    "        if not os.path.exists(save_folder):\n",
    "            os.makedirs(save_folder)\n",
    "        # save with random name\n",
    "        plt.savefig(save_folder + \"/\" + str(random.randint(0, 1000000)) + \".png\")\n",
    "        plt.close()\n",
    "\n",
    "def calculate_percentage(folder_path):\n",
    "    total_count = 0\n",
    "    meaningful_count = 0\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".png\"):\n",
    "            total_count += 1\n",
    "            if filename[-5] == \"y\":\n",
    "                meaningful_count += 1\n",
    "            elif filename[-5] == \"n\":\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Warning: unexpected filename {filename}\")\n",
    "    if total_count == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return meaningful_count / total_count * 100\n",
    "\n",
    "def entropy(lst):\n",
    "    n = len(lst)\n",
    "    counts = {}\n",
    "    for x in lst:\n",
    "        counts[x] = counts.get(x, 0) + 1\n",
    "    probs = [count / n for count in counts.values()]\n",
    "    return -sum(p * math.log2(p) for p in probs)\n",
    "\n",
    "def entropy_one_hot(list):\n",
    "    # print('entropy_one_hot list',list)\n",
    "    # Convert the list to a numpy array\n",
    "    if len(list) == 0:\n",
    "        return 0\n",
    "    arr = np.array(list)\n",
    "    # Find the number of rows and columns\n",
    "    n_rows, n_cols = arr.shape\n",
    "    # Initialize the entropy to zero\n",
    "    ent = 0\n",
    "    # Loop over each column\n",
    "    for i in range(n_cols):\n",
    "        # Extract the column as a vector\n",
    "        col = arr[:, i]\n",
    "        # Count the number of ones and zeros\n",
    "        ones = np.count_nonzero(col)\n",
    "        zeros = n_rows - ones\n",
    "        # Calculate the probabilities of ones and zeros\n",
    "        p_ones = ones / n_rows\n",
    "        p_zeros = zeros / n_rows\n",
    "        # Check if the probabilities are nonzero\n",
    "        if p_ones > 0 and p_zeros > 0:\n",
    "            # Add the entropy contribution of this column\n",
    "            ent += -p_ones * log(p_ones, e) - p_zeros * log(p_zeros, e)\n",
    "    # Return the entropy\n",
    "    return ent\n",
    "\n",
    "def calculate_graph_stats(graph):\n",
    "    nodes_data = pd.DataFrame(graph.nodes(data=True), columns=[\"node\", \"data\"])\n",
    "    edges_data = pd.DataFrame(\n",
    "        graph.edges(data=True), columns=[\"source\", \"target\", \"data\"]\n",
    "    )\n",
    "    nodes_data_df = pd.DataFrame(graph.nodes(data=True), columns=[\"node\", \"data\"])\n",
    "    entity_count = (\n",
    "        nodes_data_df[\"data\"].apply(lambda x: x.get(\"entity_conf\") == 1).sum()\n",
    "    )\n",
    "    behave_count = (\n",
    "        edges_data[\"data\"]\n",
    "        .apply(lambda x: x.get(\"behave_conf\") and x.get(\"behave_conf\") > 0.5)\n",
    "        .sum()\n",
    "    )\n",
    "    tactic_count = (\n",
    "        edges_data[\"data\"]\n",
    "        .apply(lambda x: x.get(\"tactic_conf\") and x.get(\"tactic_conf\") != [0] * 10)\n",
    "        .sum()\n",
    "    )\n",
    "    entity_percent = entity_count / len(graph.nodes()) if len(graph.nodes()) > 0 else 0\n",
    "    behave_percent = behave_count / len(graph.edges()) if len(graph.edges()) > 0 else 0\n",
    "    tactic_percent = tactic_count / len(graph.edges()) if len(graph.edges()) > 0 else 0\n",
    "    avg_precent = (\n",
    "        (entity_percent + behave_percent + tactic_percent) / 3\n",
    "        if len(graph.nodes()) > 0\n",
    "        else 0\n",
    "    )\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"entity_percent\": [entity_percent],\n",
    "            \"behave_percent\": [behave_percent],\n",
    "            \"tactic_percent\": [tactic_percent],\n",
    "        }\n",
    "    )\n",
    "    all_article_id = edges_data[\"data\"].apply(lambda x: x.get(\"article_id\")).tolist()\n",
    "    all_tactic = edges_data[\"data\"].apply(lambda x: x.get(\"tactic_conf\")).tolist()\n",
    "    article_entropy = entropy(all_article_id)\n",
    "    tactic_entropy = entropy_one_hot(all_tactic)\n",
    "    return (\n",
    "        avg_precent,\n",
    "        entity_percent,\n",
    "        behave_percent,\n",
    "        tactic_percent,\n",
    "        article_entropy,\n",
    "        tactic_entropy,\n",
    "    )\n",
    "\n",
    "def calculate_community_scores(listofcommunities, inputG):\n",
    "    df_community_and_scores = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"community_nodes\",\n",
    "            \"entity_percent\",\n",
    "            \"behave_percent\",\n",
    "            \"tactic_percent\",\n",
    "            \"avg_precent\",\n",
    "            \"article_entropy\",\n",
    "        ]\n",
    "    )\n",
    "    # shuffle the listofcommunities\n",
    "    listofcommunities = listofcommunities.copy()\n",
    "    np.random.shuffle(listofcommunities)\n",
    "    for one_community in (listofcommunities):\n",
    "        graph_one_community = create_new_graph(one_community, inputG)\n",
    "\n",
    "        (\n",
    "            avg_precent,\n",
    "            entity_percent,\n",
    "            behave_percent,\n",
    "            tactic_percent,\n",
    "            article_entropy,\n",
    "            tactic_entropy,\n",
    "        ) = calculate_graph_stats(graph_one_community)\n",
    "\n",
    "        df_community_and_scores = pd.concat(\n",
    "            [\n",
    "                df_community_and_scores,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"community_nodes\": [one_community],\n",
    "                        \"entity_percent\": [entity_percent],\n",
    "                        \"behave_percent\": [behave_percent],\n",
    "                        \"tactic_percent\": [tactic_percent],\n",
    "                        \"avg_precent\": [avg_precent],\n",
    "                        \"article_entropy\": [article_entropy],\n",
    "                        \"tactic_entropy\": [tactic_entropy],\n",
    "                    }\n",
    "                ),\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    min_x = df_community_and_scores[\"article_entropy\"].min()\n",
    "    max_x = df_community_and_scores[\"article_entropy\"].max()\n",
    "    df_community_and_scores[\"normalized_article_entropy\"] = 0\n",
    "    for i, row in df_community_and_scores.iterrows():\n",
    "        x_i = row[\"article_entropy\"]\n",
    "        z_i = (x_i - min_x) / (max_x - min_x)\n",
    "        df_community_and_scores.loc[i, \"normalized_article_entropy\"] = z_i\n",
    "\n",
    "    min_x = df_community_and_scores[\"tactic_entropy\"].min()\n",
    "    max_x = df_community_and_scores[\"tactic_entropy\"].max()\n",
    "    df_community_and_scores[\"normalized_tactic_entropy\"] = 0\n",
    "    for i, row in df_community_and_scores.iterrows():\n",
    "        x_i = row[\"tactic_entropy\"]\n",
    "        if max_x == min_x:\n",
    "                    z_i = 0\n",
    "        else:\n",
    "            z_i = (x_i - min_x) / (max_x - min_x)        \n",
    "        df_community_and_scores.loc[i, \"normalized_tactic_entropy\"] = z_i\n",
    "\n",
    "    df_community_and_scores[\"avg_score\"] = (\n",
    "        df_community_and_scores[\"entity_percent\"]\n",
    "        + df_community_and_scores[\"behave_percent\"]\n",
    "        + df_community_and_scores[\"tactic_percent\"]\n",
    "        + df_community_and_scores[\"normalized_article_entropy\"]\n",
    "        + df_community_and_scores[\"normalized_tactic_entropy\"]\n",
    "    ) / 5\n",
    "\n",
    "    df_community_and_scores = df_community_and_scores.sort_values(\n",
    "        by=[\"avg_score\"], ascending=False\n",
    "    )\n",
    "\n",
    "    min_x = df_community_and_scores[\"avg_score\"].min()\n",
    "    max_x = df_community_and_scores[\"avg_score\"].max()\n",
    "    df_community_and_scores[\"normalized_avg_score\"] = 0\n",
    "    for i, row in df_community_and_scores.iterrows():\n",
    "        x_i = row[\"avg_score\"]\n",
    "        if max_x == min_x:\n",
    "            z_i = 0\n",
    "        else:\n",
    "            z_i = (x_i - min_x) / (max_x - min_x)\n",
    "        df_community_and_scores.loc[i, \"normalized_avg_score\"] = z_i\n",
    "\n",
    "    return df_community_and_scores\n",
    "\n",
    "def create_new_graph(node_list, graph):\n",
    "    new_graph = graph.subgraph(node_list).copy()\n",
    "    isolated_nodes = list(nx.isolates(new_graph))\n",
    "    new_graph.remove_nodes_from(isolated_nodes)\n",
    "    return new_graph\n",
    "\n",
    "def OLD_create_new_graph(node_list, graph):\n",
    "    new_graph = graph.subgraph(node_list).copy()\n",
    "    if not new_graph.nodes:\n",
    "        return None\n",
    "    edges_to_remove = []\n",
    "    for edge in new_graph.edges:\n",
    "        if edge[0] not in node_list or edge[1] not in node_list:\n",
    "            edges_to_remove.append(edge)\n",
    "    new_graph.remove_edges_from(edges_to_remove)\n",
    "    isolated_nodes = list(nx.isolates(new_graph))\n",
    "    new_graph.remove_nodes_from(isolated_nodes)\n",
    "    return new_graph\n",
    "\n",
    "def OLD_calculate_graph_stats(graph):\n",
    "    entity_count = 0\n",
    "    behave_count = 0\n",
    "    tactic_count = 0\n",
    "    all_article_id = []\n",
    "    all_tactic = []\n",
    "    total_edges_from_all_articles = 0\n",
    "    for node in graph.nodes(data=True):\n",
    "        if node[1].get(\"entity\") and node[1].get(\"entity\").strip() != \"\":\n",
    "            entity_count += 1\n",
    "    for edge in graph.edges(data=True):\n",
    "        raw_sent_this = edge[2].get(\"raw_sent\")\n",
    "        total_edges_from_all_articles = total_edges_from_all_articles + 1\n",
    "        all_tactic.append(edge[2].get(\"tactic_conf\"))\n",
    "        all_article_id.append(edge[2].get(\"article_id\"))\n",
    "        if edge[2].get(\"behave_conf\") and edge[2].get(\"behave_conf\") > 0.5:\n",
    "            behave_count += 1\n",
    "        if edge[2].get(\"tactic_conf\") and edge[2].get(\"tactic_conf\") != [0] * 10:\n",
    "            tactic_count += 1\n",
    "    entity_percent = entity_count / len(graph.nodes())\n",
    "    behave_percent = behave_count / len(graph.edges()) if len(graph.edges()) > 0 else 0\n",
    "    tactic_percent = tactic_count / len(graph.edges()) if len(graph.edges()) > 0 else 0\n",
    "    avg_precent = (\n",
    "        (entity_percent + behave_percent + tactic_percent) / 3\n",
    "        if len(graph.nodes()) > 0\n",
    "        else 0\n",
    "    )\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"entity_percent\": [entity_percent],\n",
    "            \"behave_percent\": [behave_percent],\n",
    "            \"tactic_percent\": [tactic_percent],\n",
    "        }\n",
    "    )\n",
    "    article_entropy = entropy(all_article_id)\n",
    "    tactic_entropy = entropy_one_hot(all_tactic)\n",
    "    return (\n",
    "        avg_precent,\n",
    "        entity_percent,\n",
    "        behave_percent,\n",
    "        tactic_percent,\n",
    "        article_entropy,\n",
    "        tactic_entropy,\n",
    "    )\n",
    "\n",
    "def get_community_size(folder_name):\n",
    "    return int(folder_name.split(\"_\")[-1])\n",
    "\n",
    "def own_score_erdos_renyi(in_graph, in_comms_list):\n",
    "    if type(in_comms_list)!=list:\n",
    "        in_comms_list=in_comms_list.communities\n",
    "    m = in_graph.number_of_edges()\n",
    "    n = in_graph.number_of_nodes()\n",
    "    q = 0\n",
    "\n",
    "    for community in in_comms_list:\n",
    "        c = nx.subgraph(in_graph, community)\n",
    "        mc = c.number_of_edges()\n",
    "        nc = c.number_of_nodes()\n",
    "        q += mc - (m * nc * (nc - 1)) / (n * (n - 1))\n",
    "\n",
    "    return (1 / m) * q\n",
    "\n",
    "def own_score_z(graph,comms_list):\n",
    "    m = graph.number_of_edges()\n",
    "    mmc = 0\n",
    "    dc2m = 0\n",
    "    for community in comms_list:\n",
    "        c = nx.subgraph(graph, community)\n",
    "        mc = c.number_of_edges()\n",
    "        dc = 0\n",
    "        for node in c:\n",
    "            dc += graph.degree(node)\n",
    "        mmc += mc / m\n",
    "        dc2m += (dc / (2 * m)) ** 2\n",
    "    res = 0\n",
    "    try:\n",
    "        res = (mmc - dc2m) / np.sqrt(dc2m * (1 - dc2m))\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "    return res\n",
    "\n",
    "def own_modularity_overlap(graph: nx.Graph, comms_list,weight=None):\n",
    "    from collections import defaultdict\n",
    "    affiliation_dict = defaultdict(list)\n",
    "    for cid, coms in enumerate(comms_list):\n",
    "        for n in coms:\n",
    "            affiliation_dict[n].append(cid)\n",
    "    mOvTotal = 0\n",
    "    for nodes in comms_list:\n",
    "        nCommNodes = len(nodes)\n",
    "        # the contribution of communities with 1 node is 0\n",
    "        if nCommNodes <= 1:\n",
    "            continue\n",
    "        nInwardEdges = 0\n",
    "        commStrength = 0\n",
    "        for node in nodes:\n",
    "            degree, inwardEdges, outwardEdges = 0, 0, 0\n",
    "            for u, v, data in graph.edges(node, data=True):\n",
    "                w = data.get(weight, 1)\n",
    "                degree += w\n",
    "                if v in nodes:\n",
    "                    inwardEdges += w\n",
    "                    nInwardEdges += 1\n",
    "                else:\n",
    "                    outwardEdges += w\n",
    "            affiliationCount = len(affiliation_dict[node])\n",
    "            denom = degree * affiliationCount\n",
    "            if denom > 0:\n",
    "                commStrength += (inwardEdges - outwardEdges) / denom\n",
    "        binomC = nCommNodes * (nCommNodes - 1)\n",
    "        v1 = commStrength / nCommNodes\n",
    "        v2 = nInwardEdges / binomC\n",
    "        mOv = v1 * v2\n",
    "        mOvTotal += mOv\n",
    "    score = mOvTotal / len(comms_list)\n",
    "    return score\n",
    "\n",
    "#print(own_modularity_overlap(G_Value_All_tactic,communities['ownAlgorithm_threshold_0.2.pkl'])\n",
    "\n",
    "\n",
    "def own_modularity_density(\n",
    "    graph: nx.Graph, comms_list: object, lmbd: float = 0.5, **kwargs: dict\n",
    "):\n",
    "    q = 0\n",
    "\n",
    "    for community in comms_list:\n",
    "        c = nx.subgraph(graph, community)\n",
    "\n",
    "        nc = c.number_of_nodes()\n",
    "        dint = []\n",
    "        dext = []\n",
    "        for node in c:\n",
    "            dint.append(c.degree(node))\n",
    "            dext.append(graph.degree(node) - c.degree(node))\n",
    "\n",
    "        try:\n",
    "            q += (1 / nc) * (\n",
    "                (2 * lmbd * np.sum(dint)) - (2 * (1 - lmbd) * np.sum(dext))\n",
    "            )\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "\n",
    "    return q\n",
    "\n",
    "def own_newman_girvan_modularity(\n",
    "    graph: nx.Graph, comms_list: object, **kwargs: dict\n",
    ") -> object:\n",
    "    coms = {}\n",
    "    for cid, com in enumerate(comms_list):\n",
    "        for node in com:\n",
    "            coms[node] = cid\n",
    "\n",
    "    inc = dict([])\n",
    "    deg = dict([])\n",
    "    links = graph.size(weight=\"weight\")\n",
    "    if links == 0:\n",
    "        raise ValueError(\"A graph without link has an undefined modularity\")\n",
    "\n",
    "    for node in graph:\n",
    "        try:\n",
    "            com = coms[node]\n",
    "            deg[com] = deg.get(com, 0.0) + graph.degree(node, weight=\"weight\")\n",
    "            for neighbor, dt in graph[node].items():\n",
    "                weight = dt.get(\"weight\", 1)\n",
    "                if coms[neighbor] == com:\n",
    "                    if neighbor == node:\n",
    "                        inc[com] = inc.get(com, 0.0) + float(weight)\n",
    "                    else:\n",
    "                        inc[com] = inc.get(com, 0.0) + float(weight) / 2.0\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    res = 0.0\n",
    "    for com in set(coms.values()):\n",
    "        res += (inc.get(com, 0.0) / links) - (deg.get(com, 0.0) / (2.0 * links)) ** 2\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "import random\n",
    "from collections import deque\n",
    "import networkx as nx\n",
    "import os\n",
    "def clear_file(file_path):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "    with open(file_path, 'w') as f:\n",
    "        pass\n",
    "\n",
    "def NodeCTI_GetEdgesIntoDict(inputG):\n",
    "    edges = {}\n",
    "    for t in inputG.edges():\n",
    "        if len(t) > 0:\n",
    "            if t[0] != t[1]:\n",
    "                if t[0] not in edges:\n",
    "                    edges[t[0]] = {t[1]}\n",
    "                else:\n",
    "                    edges[t[0]].add(t[1])\n",
    "                if t[1] not in edges:\n",
    "                    edges[t[1]] = {t[0]}\n",
    "                else:\n",
    "                    edges[t[1]].add(t[0])\n",
    "    return edges\n",
    "\n",
    "def NodeCTI_FirstPartition(edges, first_part_file):\n",
    "    OUT = open(first_part_file, \"w\")\n",
    "    node_count = 0\n",
    "    for n in edges:\n",
    "        node_count = node_count + 1\n",
    "        if node_count > 0:\n",
    "            index = {}\n",
    "            reverse_index = {}\n",
    "            count = 0\n",
    "            to_add_edges = []\n",
    "            adj = set([])\n",
    "            for neighbor in edges[n]:\n",
    "                index[count] = neighbor\n",
    "                reverse_index[neighbor] = count\n",
    "                adj.add(neighbor)\n",
    "                count = count + 1\n",
    "            for m in reverse_index:\n",
    "                for k in edges[m]:\n",
    "                    if k in reverse_index and reverse_index[k] < reverse_index[m]:\n",
    "                        to_add_edges.append((reverse_index[m], reverse_index[k]))\n",
    "            G = nx.Graph()\n",
    "            G.add_nodes_from([i for i in index])\n",
    "            G.add_edges_from(to_add_edges)\n",
    "            if len(to_add_edges) > 0:\n",
    "                dict_H = community.best_partition(G)\n",
    "                H = {}\n",
    "                for node in dict_H:\n",
    "                    if dict_H[node] not in H:\n",
    "                        H[dict_H[node]] = set([])\n",
    "                    H[dict_H[node]].add(node)\n",
    "                for i in H:\n",
    "                    comm = H[i]\n",
    "                    if len(comm) > 0:\n",
    "                        for c in comm:\n",
    "                            OUT.write(str(index[int(c)]) + \" \")\n",
    "                        OUT.write(str(n))\n",
    "                        OUT.write(\"\\n\")\n",
    "                    elif len(comm) > 0:\n",
    "                        for c in comm:\n",
    "                            if index[int(c)] in edges[n]:\n",
    "                                OUT.write(str(index[int(c)]) + \" \")\n",
    "                        OUT.write(str(n))\n",
    "                        OUT.write(\"\\n\")\n",
    "    OUT.close()\n",
    "\n",
    "def NodeCTI_Jaccard(set1, set2):\n",
    "        set1 = set(set1)\n",
    "        set2 = set(set2)\n",
    "        return float(len(set1.intersection(set2))) / float(len(set1.union(set2)))\n",
    "\n",
    "def NodeCTI_GetMembership(first_part_file, membership_file):\n",
    "    node_membership = {}\n",
    "    IN = open(first_part_file, \"rb\")\n",
    "    read_line = IN.readline()\n",
    "    count = 0\n",
    "    while read_line:\n",
    "        t = read_line.rstrip().split()\n",
    "        if len(t) >= min_comm_size:\n",
    "            for mem in t:\n",
    "                if mem not in node_membership:\n",
    "                    node_membership[mem] = set([])\n",
    "                node_membership[mem].add(count)\n",
    "        count = count + 1\n",
    "        read_line = IN.readline()\n",
    "\n",
    "    IN.close()\n",
    "\n",
    "    OUT = open(membership_file, \"w\")\n",
    "    for n in node_membership:\n",
    "        in_comms = node_membership[n]\n",
    "        OUT.write(str(n) + \" \")\n",
    "        for c in in_comms:\n",
    "            OUT.write(str(c) + \" \")\n",
    "        OUT.write(\"\\n\")\n",
    "    OUT.close()\n",
    "\n",
    "def get_subset_graph(in_undirected_G, percent_to_keep):\n",
    "    num_nodes = len(in_undirected_G.nodes())\n",
    "    num_edges = len(in_undirected_G.edges())\n",
    "    num_nodes_to_keep = int(num_nodes * percent_to_keep)\n",
    "    num_edges_to_keep = int(num_edges * percent_to_keep)\n",
    "    nodes_to_keep = random.sample(list(in_undirected_G.nodes()), num_nodes_to_keep)\n",
    "    edges_to_keep = random.sample(list(in_undirected_G.edges()), num_edges_to_keep)\n",
    "    G_Value_undirected_subset = nx.Graph()\n",
    "    G_Value_undirected_subset.add_nodes_from(nodes_to_keep)\n",
    "    G_Value_undirected_subset.add_edges_from(edges_to_keep)\n",
    "    return G_Value_undirected_subset\n",
    "\n",
    "###2.1\n",
    "import random\n",
    "import networkx as nx\n",
    "first_part_file = \"./tmp/part1_.txt\"\n",
    "membership_file = \"./tmp/membership_.txt\"\n",
    "sim_file = \"./tmp/simfile_.txt\"\n",
    "clear_file(first_part_file)\n",
    "clear_file(membership_file)\n",
    "clear_file(sim_file)\n",
    "G_Value_undirected_subset=G_Value_undirected\n",
    "edges = NodeCTI_GetEdgesIntoDict(G_Value_undirected_subset)\n",
    "min_comm_size=3\n",
    "sim_threshold=0.25\n",
    "global_overlap_threshold=0.25\n",
    "NodeCTI_FirstPartition(edges,first_part_file)\n",
    "\n",
    "def NodeCTI_Filter_only_MalwareCVEActor(file_path, in_entity_set):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        line = line.strip().split(' ')\n",
    "        if line[-1] in in_entity_set:\n",
    "           # print(line[-1])\n",
    "            filtered_lines.append(line)\n",
    "    #save original file as backup\n",
    "    #if backup file exists, delete it\n",
    "    if os.path.exists(file_path + '.bak'):\n",
    "        os.remove(file_path + '.bak')\n",
    "    with open(file_path + '.bak', 'w') as f:\n",
    "        for line in lines:\n",
    "            f.write(line)\n",
    "    #save back to original file\n",
    "    with open(file_path, 'w') as f:\n",
    "        for line in filtered_lines:\n",
    "            f.write(' '.join(line) + '\\n')\n",
    "    #output the length difference between original and filtered file\n",
    "    print('The length of original file:', len(lines), 'The length of filtered file:', len(filtered_lines), 'The difference:', len(lines) - len(filtered_lines))\n",
    "\n",
    "entity_conf_set = set()\n",
    "\n",
    "for node in G_Value.nodes():\n",
    "    #if G_Value.nodes[node]['entity']=='malware' or G_Value.nodes[node]['entity']=='cve' or G_Value.nodes[node]['entity']=='CVE':\n",
    "    if G_Value.nodes[node]['entity']!='':\n",
    "        entity_conf_set.add(node)\n",
    "\n",
    "entity_conf_list = list(entity_conf_set)\n",
    "\n",
    "NodeCTI_Filter_only_MalwareCVEActor(first_part_file, entity_conf_set)\n",
    "NodeCTI_GetMembership(first_part_file, membership_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")\n",
    "\n",
    "with open('./tmp/part1_.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "num_tokens = 0\n",
    "for line in lines:\n",
    "    num_tokens += num_tokens_from_string(line, \"cl100k_base\")\n",
    "    \n",
    "print(num_tokens)\n",
    "print(num_tokens/1000*0.0004)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(lines, columns=['data'])\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"<APIKeyHere>\"\n",
    "\n",
    "with open('./part1_.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "embeddings = OpenAIEmbeddings()\n",
    "lines_result=embeddings.embed_documents(lines)\n",
    "#save the result as dict then save it as pickle file\n",
    "embeddings_dict = {}\n",
    "for i in range(len(lines_result)):\n",
    "    embeddings_dict[lines[i]] = lines_result[i]\n",
    "import pickle\n",
    "with open('embedding_BY_OPENAI.pickle', 'wb') as handle:\n",
    "    pickle.dump(embeddings_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "import pickle\n",
    "with open('./part1_.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "lines_result=[]\n",
    "with open('embedding_BY_OPENAI.pickle', 'rb') as handle:\n",
    "    embeddings_dict = pickle.load(handle)\n",
    "for i in lines:\n",
    "    lines_result.append(embeddings_dict[i])\n",
    "    \n",
    "lines_result_possible=[]\n",
    "lines_result_possible_truevalue=[]\n",
    "for i in range(len(lines_result)):\n",
    "    if lines[i].count(' ')>=2:\n",
    "        lines_result_possible_truevalue.append(lines[i])\n",
    "        lines_result_possible.append(lines_result[i])\n",
    "        \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from numpy.linalg import norm\n",
    "np.random.seed(42)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "dist = pdist(lines_result_possible, metric=cosine_similarity)\n",
    "\n",
    "dist_mat = squareform(dist)\n",
    "\n",
    "sims = np.mean(dist_mat, axis=0)\n",
    "plt.hist(sims, bins=50, color='blue', edgecolor='black')\n",
    "plt.xlabel('Cosine similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Similarity distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmap(setvalue):\n",
    "    threshold_V=setvalue\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    df_sims=pd.DataFrame(sims, columns=['sims'])\n",
    "    # assuming df_sims is already defined\n",
    "\n",
    "    df_threshold=pd.DataFrame(columns=['top N%','minimum sims threshold'])\n",
    "    df_threshold['top N%']=[10,20,30,40,50]\n",
    "    df_threshold['minimum sims threshold']=[threshold_percentile(sims, 10),threshold_percentile(sims, 20),threshold_percentile(sims, 30),threshold_percentile(sims, 40),threshold_percentile(sims, 50)]\n",
    "    '''\n",
    "    def threshold_percentile(sims, percentile):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        threshold = sorted(sims, reverse=True)[int(len(sims) * percentile / 100)]\n",
    "        return threshold\n",
    "    #if 'tmp/simfile_.txt' exists, delete it\n",
    "    import os\n",
    "    if os.path.exists('tmp/simfile_.txt'):\n",
    "        os.remove('tmp/simfile_.txt')\n",
    "\n",
    "    value_p1file_index={}\n",
    "    for line_this in lines_result_possible_truevalue:\n",
    "        index_loc=lines.index(line_this)\n",
    "        value_p1file_index[line_this]=index_loc\n",
    "        \n",
    "    result_newp1=[]\n",
    "    #add tqdm for i in range(len(dist_mat)):\n",
    "    total_sim=[]\n",
    "    for i in range(len(dist_mat)):\n",
    "        for j in range(i,len(dist_mat)):\n",
    "            total_sim.append(dist_mat[i][j])\n",
    "\n",
    "    threshold_this=threshold_percentile(total_sim, threshold_V)\n",
    "    #print threshold_this\n",
    "    print('threshold_this:',threshold_this)\n",
    "    import tqdm\n",
    "    for i in tqdm.tqdm(range(len(dist_mat))):\n",
    "        for j in range(i,len(dist_mat)):\n",
    "            if i!=j:\n",
    "                #if dist_mat[i][j]>0.85:\n",
    "                if dist_mat[i][j]>threshold_this:\n",
    "                    aindex=value_p1file_index[lines_result_possible_truevalue[i]]\n",
    "                    bindex=value_p1file_index[lines_result_possible_truevalue[j]]\n",
    "                    #split a by space, b by space\n",
    "                    aindex_split=lines_result_possible_truevalue[i].split(' ')\n",
    "                    bindex_split=lines_result_possible_truevalue[j].split(' ')\n",
    "                    #remove \\n in last element of a and b\n",
    "                    aindex_split[-1]=aindex_split[-1].replace('\\n','')\n",
    "                    bindex_split[-1]=bindex_split[-1].replace('\\n','')\n",
    "                    union_len=len(set(aindex_split).intersection(set(bindex_split)))\n",
    "                    a_len=len(aindex_split)\n",
    "                    b_len=len(bindex_split)\n",
    "                    if a_len <10 and b_len <10 and union_len>=1:\n",
    "                        result_newp1.append(str(aindex)+' '+str(bindex)+' '+str(dist_mat[i][j])+' '+str(union_len)+' '+str(a_len)+' '+str(b_len)+' '+str(dist_mat[i][j]))\n",
    "    #save the result as txt file as tmp/simfile_.txt\n",
    "    with open('tmp/simfile_.txt', 'w') as f:\n",
    "        for item in result_newp1:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "            \n",
    "    def NodeCTI_SecondPartition(overlap_threshold, sim_file, first_part_file):\n",
    "            return_vals =NodeCTI_ModClusteringSingleBig(\n",
    "                sim_file, first_part_file, overlap_threshold\n",
    "            )\n",
    "            return return_vals\n",
    "\n",
    "    def NodeCTI_ModClusteringSingleBig(sim_file, first_part_file, overlap_threshold=0):\n",
    "        overlap_threshold=global_overlap_threshold\n",
    "        IN = open(sim_file, \"r\")\n",
    "        read_line = IN.readline()\n",
    "        num_lines = 0\n",
    "        comm_edges = {}\n",
    "        to_add_edges = []\n",
    "        while read_line:\n",
    "            t = read_line.rstrip().split()\n",
    "            num_lines += 1\n",
    "            if len(t) > 0:\n",
    "                node1 = int(t[0])\n",
    "                node2 = int(t[1])\n",
    "                sim = t[2]\n",
    "                overlap = t[3]\n",
    "                if node1 not in comm_edges:\n",
    "                    comm_edges[node1] = set([])\n",
    "                if node2 not in comm_edges:\n",
    "                    comm_edges[node2] = set([])\n",
    "                if node2 not in comm_edges[node1]:\n",
    "                    comm_edges[node1].add(node2)\n",
    "                    comm_edges[node2].add(node1)\n",
    "                    weight = sim\n",
    "                    to_add_edges.append((node1, node2, {\"weight\": float(weight)}))\n",
    "\n",
    "            read_line = IN.readline()\n",
    "        IN.close()\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(range(len(comm_edges)))\n",
    "        G.add_edges_from(to_add_edges)\n",
    "        dict_H = community.best_partition(G)\n",
    "        H1 = {}\n",
    "        for e in dict_H:\n",
    "            if dict_H[e] not in H1:\n",
    "                H1[dict_H[e]] = set([])\n",
    "            H1[dict_H[e]].add(e)\n",
    "        H = []\n",
    "        for e in H1:\n",
    "            H.append(H1[e])\n",
    "\n",
    "        IN = open(first_part_file, \"rb\")\n",
    "        line_offset = {}\n",
    "        offset = 0\n",
    "        count = 0\n",
    "        for line in IN:\n",
    "            line_offset[count] = offset\n",
    "            count = count + 1\n",
    "            offset += len(line)\n",
    "        IN.close()\n",
    "\n",
    "        IN = open(first_part_file, \"rb\")\n",
    "        all_comms = {}\n",
    "        i = 0\n",
    "        for big_comm in H:\n",
    "            comm_members = {}\n",
    "            for comm in big_comm:\n",
    "                IN.seek(line_offset[int(comm)])\n",
    "                read_line = IN.readline()\n",
    "                t = read_line.rstrip().split()\n",
    "                if len(t) > 0:\n",
    "                    for t1 in t:\n",
    "                        if t1 not in comm_members:\n",
    "                            comm_members[t1] = 0\n",
    "                        comm_members[t1] += 1\n",
    "            all_comms[i] = set([])\n",
    "            for t1 in comm_members:\n",
    "                if comm_members[t1] >= 0:\n",
    "                    all_comms[i].add(t1)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return all_comms\n",
    "\n",
    "    def NodeCTI_GetModComms(G):\n",
    "        dict_H = community.best_partition(G)\n",
    "        H1 = {}\n",
    "        for e in dict_H:\n",
    "            if dict_H[e] not in H1:\n",
    "                H1[dict_H[e]] = set([])\n",
    "            H1[dict_H[e]].add(e)\n",
    "        H = []\n",
    "        for e in H1:\n",
    "            H.append(H1[e])\n",
    "        return H\n",
    "\n",
    "    def NodeCTI_CleanComms(to_clean):\n",
    "        comms = {}\n",
    "        count = 0\n",
    "        idx = {}\n",
    "        for t in to_clean.values():\n",
    "            if len(t) > 0:\n",
    "                comms[count] = set(t)\n",
    "                for i in t:\n",
    "                    if i not in idx:\n",
    "                        idx[i] = set([])\n",
    "\n",
    "                    idx[i].add(count)\n",
    "                count += 1\n",
    "            elif len(t) > 0:\n",
    "                comms[count] = set(t)\n",
    "                count += 1\n",
    "        coms = []\n",
    "        for i in range(count):\n",
    "            C = comms[i]\n",
    "            if len(C) > 0:\n",
    "                poss = set([])\n",
    "                found = 0\n",
    "                for n in C:\n",
    "                    poss = poss.union(idx[n])\n",
    "                for j in poss:\n",
    "                    if j < i:\n",
    "                        if (\n",
    "                            len(comms[j]) == len(comms[i])\n",
    "                            and len(comms[j].difference(comms[i])) == 0\n",
    "                        ):\n",
    "                            found = 1\n",
    "                if found != 1:\n",
    "                    coms.append([t.decode(\"utf-8\") for t in C])\n",
    "            else:\n",
    "                coms.append([t.decode(\"utf-8\") for t in C])\n",
    "        return coms\n",
    "\n",
    "    import networkx as nx\n",
    "    import community\n",
    "    sim_file='tmp/simfile_.txt'\n",
    "    first_part_file='tmp/part1_.txt'\n",
    "    global_overlap_threshold=0\n",
    "    return_vals = NodeCTI_SecondPartition(0, sim_file, first_part_file)\n",
    "    coms = NodeCTI_CleanComms(return_vals)\n",
    "    print('Find communities number:',len(coms))\n",
    "\n",
    "    #save as pkl file name \"ownAlgorithm_threshold_\"+str(threshold_V)+\".pkl\"\n",
    "    import pickle\n",
    "    fianlname='ownAlgorithm_threshold_'+str(threshold_V)+'.pkl'\n",
    "    with open(fianlname, 'wb') as f:\n",
    "        pickle.dump(coms, f)\n",
    "    print('save as pkl file name:',fianlname)\n",
    "\n",
    "    import networkx as nx\n",
    "    import pickle\n",
    "    import copy\n",
    "    from cdlib import algorithms\n",
    "    import os\n",
    "    import time\n",
    "    from cdlib import algorithms\n",
    "    with open('./SECCG generated Knowledge Graph.pkl', 'rb') as f:\n",
    "        G_Value = pickle.load(f)\n",
    "    undirected_G_Value=copy.deepcopy(G_Value)\n",
    "    undirected_G_Value=G_Value.to_undirected()\n",
    "    G_Value_undirected=undirected_G_Value.to_undirected()\n",
    "\n",
    "    import networkx as nx\n",
    "\n",
    "\n",
    "    nx.set_edge_attributes(G_Value, name=\"tactic_conf\", values=nx.get_edge_attributes(G_Value, \"tactic\"))\n",
    "\n",
    "    import networkx as nx\n",
    "    nx.set_edge_attributes(G_Value, name=\"relation\", values=nx.get_edge_attributes(G_Value, \"rel\"))\n",
    "    \n",
    "    return coms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coms_sizein5to10=[i for i in coms if len(i)>=5 and len(i)<=10]\n",
    "df=pd.DataFrame(columns=['value','Average Size','Average Edge','Average Articles','Average Linked CTI Node'])\n",
    "for i in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    coms_sizein5to10=getmap(i)\n",
    "    big_label_list=['Initial Access', 'Execution', 'Defense Evasion', 'Command and Control', 'Privilege Escalation', 'Persistence','Lateral Movement','DataLeak','Exfiltration','Impact']\n",
    "    import random\n",
    "    average_size=[]\n",
    "    #add tqdm for community in coms_sizein5to10:\n",
    "    import tqdm\n",
    "    average_edge=[]\n",
    "    average_linked_cti_edge=[]\n",
    "    average_article=[]\n",
    "    import networkx as nx\n",
    "\n",
    "    def count_edges_with_nonempty_entity(graph):\n",
    "        count = 0\n",
    "\n",
    "        for edge in graph.edges():\n",
    "            node1, node2 = edge\n",
    "\n",
    "            if graph.nodes[node1]['entity'] != '' and graph.nodes[node2]['entity'] != '':\n",
    "                count += 1\n",
    "\n",
    "        return count\n",
    "    for community in tqdm.tqdm(coms_sizein5to10):\n",
    "        graph_com=create_new_graph(community,G_Value)\n",
    "        if graph_com.number_of_edges()>2 and graph_com.number_of_nodes()>=1 :\n",
    "                import networkx as nx\n",
    "                edge_attributes = nx.get_edge_attributes(graph_com, 'article_id')\n",
    "                unique_article_ids = len(set(edge_attributes.values()))\n",
    "                linked_cti_edge=count_edges_with_nonempty_entity(graph_com)\n",
    "                average_edge.append(graph_com.number_of_edges())\n",
    "                average_article.append(unique_article_ids)\n",
    "                average_size.append(graph_com.number_of_nodes())\n",
    "                average_linked_cti_edge.append(linked_cti_edge)\n",
    "                \n",
    "    print('average_size:',sum(average_size)/len(average_size))\n",
    "    print('average_edge:',sum(average_edge)/len(average_edge))\n",
    "    print('average_article:',sum(average_article)/len(average_article))\n",
    "    print('average_linked_cti_edge:',sum(average_linked_cti_edge)/len(average_linked_cti_edge))\n",
    "    #contact the df\n",
    "    new_row = {'value':i,'Average Size':sum(average_size)/len(average_size),'Average Edge':sum(average_edge)/len(average_edge),'Average Articles':sum(average_article)/len(average_article),'Average Linked CTI Node':sum(average_linked_cti_edge)/len(average_linked_cti_edge)}\n",
    "    df=pd.concat([df,pd.DataFrame([new_row])])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['font.size'] = 20\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "for column in df.columns[1:]:\n",
    "    plt.plot(df['value'], df[column], marker='o', label=column)\n",
    "\n",
    "plt.title('Community Metrics with Different Thresholds')\n",
    "plt.xlabel('Threshold Value')\n",
    "plt.ylabel('Metric Valve')\n",
    "plt.xticks(df['value'])\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./SECCG generated Knowledge Graph.pkl', 'rb') as f:\n",
    "    G_full = pickle.load(f)\n",
    "import copy\n",
    "undirected_G_Value_full = copy.deepcopy(G_full)\n",
    "dict_sol_com = {}\n",
    "dict_sol_com['CTIKG0.5'] = getmap(0.5)\n",
    "undirected_G_Value_full = undirected_G_Value_full.to_undirected()\n",
    "\n",
    "# Create a new undirected graph\n",
    "undirected_G_Value_full_foUmstmo = nx.Graph()\n",
    "\n",
    "# Copy all edges into a list to avoid changing the graph structure during iteration\n",
    "edges_list = list(undirected_G_Value_full.edges())\n",
    "\n",
    "# Iterate through each set of edges between node pairs\n",
    "for u, v in edges_list:\n",
    "    # Get all edges between u and v\n",
    "    edges = list(undirected_G_Value_full.get_edge_data(u, v).values())\n",
    "    \n",
    "    # Randomly select one edge\n",
    "    selected_edge = random.choice(edges)\n",
    "    \n",
    "    # Add it to the new undirected graph\n",
    "    undirected_G_Value_full_foUmstmo.add_edge(u, v, **selected_edge)\n",
    "\n",
    "# Now undirected_G_Value_full_foUmstmo is a copy of undirected_G_Value_full with random edges selected\n",
    "\n",
    "dict_sol_com['Core'] = algorithms.core_expansion(undirected_G_Value_full).communities\n",
    "dict_sol_com['leiden'] = algorithms.leiden(undirected_G_Value_full).communities\n",
    "print('leiden done')\n",
    "dict_sol_com['Umstmo'] = algorithms.umstmo(undirected_G_Value_full_foUmstmo).communities\n",
    "dict_sol_com['angel'] = algorithms.angel(undirected_G_Value_full, threshold=0.25).communities\n",
    "dict_sol_com['coach'] = algorithms.coach(undirected_G_Value_full).communities\n",
    "\n",
    "dict_sol_com['Umstmo'] = algorithms.umstmo(undirected_G_Value_full_foUmstmo).communities\n",
    "dict_sol_com['angel'] = algorithms.angel(undirected_G_Value_full, threshold=0.25).communities\n",
    "dict_sol_com['coach'] = algorithms.coach(undirected_G_Value_full).communities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coms_sizein5to10 = [i for i in coms if len(i) >= 5 and len(i) <= 10]\n",
    "def get_table_on_names(coms_this, name):\n",
    "    df = pd.DataFrame(columns=['name', 'Average Size', 'Average Edge', 'Average Articles', 'Average Linked CTI Node', 'Number of Communities'])\n",
    "    big_label_list = ['Initial Access', 'Execution', 'Defense Evasion', 'Command and Control', 'Privilege Escalation', 'Persistence', 'Lateral Movement', 'DataLeak', 'Exfiltration', 'Impact']\n",
    "    import random\n",
    "    average_size = []\n",
    "    \n",
    "    # Add tqdm for community in coms_sizein5to10:\n",
    "    import tqdm\n",
    "    average_edge = []\n",
    "    average_linked_cti_edge = []\n",
    "    average_article = []\n",
    "    import networkx as nx\n",
    "    \n",
    "    def count_edges_with_nonempty_entity(graph):\n",
    "        # Initialize counter\n",
    "        count = 0\n",
    "\n",
    "        # Iterate over each edge in the graph\n",
    "        for edge in graph.edges():\n",
    "            # Get the two endpoints of the edge\n",
    "            node1, node2 = edge\n",
    "\n",
    "            # Check if the 'entity' attribute of both endpoints is not an empty string\n",
    "            if graph.nodes[node1]['entity'] != '' and graph.nodes[node2]['entity'] != '':\n",
    "                # Increment counter\n",
    "                count += 1\n",
    "\n",
    "        # Return the result\n",
    "        return count\n",
    "    \n",
    "    for community in tqdm.tqdm(coms_this):\n",
    "        if 'CTIKG' in name:\n",
    "            print('Use CTIKG remove graph')\n",
    "            graph_com = create_new_graph(community, G_Value)\n",
    "        else:\n",
    "            print('Use full graph')\n",
    "            graph_com = create_new_graph(community, G_full)\n",
    "            \n",
    "        if graph_com.number_of_edges() > 2 and graph_com.number_of_nodes() >= 1:\n",
    "            edge_attributes = nx.get_edge_attributes(graph_com, 'article_id')\n",
    "            unique_article_ids = len(set(edge_attributes.values()))\n",
    "            linked_cti_edge = count_edges_with_nonempty_entity(graph_com)\n",
    "            average_edge.append(graph_com.number_of_edges())\n",
    "            average_article.append(unique_article_ids)\n",
    "            average_size.append(graph_com.number_of_nodes())\n",
    "            average_linked_cti_edge.append(linked_cti_edge)\n",
    "    \n",
    "    # Print unique value counts\n",
    "    print('average_size:', sum(average_size) / len(average_size))\n",
    "    print('average_edge:', sum(average_edge) / len(average_edge))\n",
    "    print('average_article:', sum(average_article) / len(average_article))\n",
    "    print('average_linked_cti_edge:', sum(average_linked_cti_edge) / len(average_linked_cti_edge))\n",
    "    \n",
    "    new_row = {\n",
    "        'name': name,\n",
    "        'Average Size': sum(average_size) / len(average_size),\n",
    "        'Average Edge': sum(average_edge) / len(average_edge),\n",
    "        'Average Articles': sum(average_article) / len(average_article),\n",
    "        'Average Linked CTI Node': sum(average_linked_cti_edge) / len(average_linked_cti_edge),\n",
    "        'Number of Communities': len(coms_this)\n",
    "    }\n",
    "    \n",
    "    df = pd.concat([df, pd.DataFrame([new_row])])\n",
    "    return df\n",
    "\n",
    "df_overall = []\n",
    "for key, value in dict_sol_com.items():\n",
    "    print(key)\n",
    "    try:\n",
    "        df = get_table_on_names(value, key)\n",
    "        df_overall.append(df)\n",
    "        print(df)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df_overall = pd.concat(df_overall)\n",
    "df_overall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
