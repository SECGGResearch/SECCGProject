{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9723eca6",
   "metadata": {},
   "source": [
    "Split input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Input_Data_Frame=pd.read_csv(\"1000_articles_of_100CVEs_in_2024.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122e0ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Split_length = 700\n",
    "from semantic_text_splitter import TextSplitter # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "\n",
    "splitter = TextSplitter.from_tiktoken_model(\"gpt-4\", Split_length)\n",
    "\n",
    "Core_DataFrame = pd.DataFrame()\n",
    "\n",
    "total_overlong = 0\n",
    "\n",
    "for i, row in Input_Data_Frame.iterrows():\n",
    "    content_parts = splitter.chunks(row['content'])\n",
    "    \n",
    "    temp_df = pd.DataFrame({\n",
    "        'content part order': range(0, len(content_parts)),\n",
    "        'content part': content_parts,\n",
    "        'content': row['content']\n",
    "    })\n",
    "    \n",
    "    for col in Input_Data_Frame.columns:\n",
    "        if col != 'content':\n",
    "            temp_df[col] = [row[col]] * len(temp_df)\n",
    "    \n",
    "    if len(temp_df) > 1:\n",
    "        total_overlong += 1\n",
    "\n",
    "    Core_DataFrame = pd.concat([Core_DataFrame, temp_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b44bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from IPython.display import clear_output, display, HTML\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def download_jsonl(batchid, printstatus=True):\n",
    "    # Download the result of the jsonl file based on the file id, and the result in the 'answer' key is stored as a list\n",
    "    import os         \n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"<APIKEYHERE>\"\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    api_base = 'https://api.openai.com/v1'\n",
    "    client = OpenAI(api_key=api_key, base_url=api_base)\n",
    "    # Retrieve batch status\n",
    "    full_status = client.batches.retrieve(batchid)\n",
    "    status = full_status.status\n",
    "    failed_count = full_status.request_counts.failed\n",
    "    if failed_count > 0:\n",
    "        print(f\"{failed_count} requests failed.\")\n",
    "        print(\"Full status:\", full_status)\n",
    "        \n",
    "    # Initialize return dictionary\n",
    "    result = {\n",
    "        \"status\": status,\n",
    "        \"answer\": None,\n",
    "        \"fullstatus\": full_status,\n",
    "        \"responsedf\": None,\n",
    "        \"fullresponse\": None,\n",
    "        \"errordf\": None,\n",
    "        \"errorid\": None\n",
    "    }\n",
    "    \n",
    "    # Print batch status\n",
    "    if printstatus:\n",
    "        print(f\"Batch status: {status}\")\n",
    "    \n",
    "    def jsonl_to_dataframe(jsonl_data):\n",
    "        import pandas as pd\n",
    "        # Split the JSONL data into individual JSON objects\n",
    "        json_objects = []\n",
    "        for line in jsonl_data.split('\\n'):\n",
    "            if line.strip():  # Ignore empty lines\n",
    "                try:\n",
    "                    json_objects.append(json.loads(line))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"JSONDecodeError: {e} for line: {line}\")\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.json_normalize(json_objects)\n",
    "        return df\n",
    "    \n",
    "    # If the status is 'completed', retrieve the file content\n",
    "    if status == 'completed':\n",
    "        outputid = full_status.output_file_id\n",
    "        file_response = client.files.content(outputid)\n",
    "        responsedf = jsonl_to_dataframe(file_response.text)\n",
    "        # Convert custom_id to int\n",
    "        responsedf['custom_id'] = responsedf['custom_id'].astype(int)\n",
    "        responsedf.sort_values(by=\"custom_id\", inplace=True)\n",
    "        responsedf.reset_index(drop=True, inplace=True)\n",
    "        responses = [responsedf.loc[x, 'response.body.choices'][0]['message']['content'] for x in range(len(responsedf))]\n",
    "        result[\"answer\"] = responses\n",
    "        result[\"responsedf\"] = responsedf\n",
    "        result[\"fullresponse\"] = file_response.text\n",
    "        print(\"File content retrieved. Please check the 'answer' key in the dictionary.\")\n",
    "    else:\n",
    "        if printstatus:\n",
    "            print(\"Batch status is not 'completed', not attempting to retrieve file content.\")\n",
    "    \n",
    "    if failed_count > 0:\n",
    "        print(f\"Since {failed_count} requests failed, additionally outputting errordf and errorid.\")\n",
    "        \n",
    "        # Retrieve the error file ID and download the content\n",
    "        errorid = full_status.error_file_id\n",
    "        file_error = client.files.content(errorid)\n",
    "        \n",
    "        # Print error file content\n",
    "        print(\"Error details:\", file_error.text)\n",
    "        \n",
    "        # Parse the error file content and extract the custom_id list\n",
    "        errordf = jsonl_to_dataframe(file_error.text)\n",
    "        result['errorid'] = errordf['custom_id'].astype(int).tolist()\n",
    "        \n",
    "        # Store the error data in the result dictionary\n",
    "        result[\"errordf\"] = errordf\n",
    "\n",
    "        \n",
    "    return result\n",
    "\n",
    "def auto_down_ans(id, max_wait_time=60, jobtype=\"chat\"):\n",
    "    max_wait_time_seconds = max_wait_time * 60  # Convert minutes to seconds\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        clear_output(wait=True)  # Clear previous output\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        elapsed_time = time.time() - start_time\n",
    "        elapsed_str = str(timedelta(seconds=int(elapsed_time)))  # Convert elapsed time to HH:MM:SS format\n",
    "\n",
    "        display(HTML(f\"<p>Current time: <b>{current_time}</b></p>\"))\n",
    "        display(HTML(f\"<p>Elapsed time: <b>{elapsed_str}</b></p>\"))\n",
    "        display(HTML('<p style=\"color:red;\">Processing...</p>'))  # Display \"Processing\" in red\n",
    "        if jobtype == \"chat\":\n",
    "            try:\n",
    "                download = download_jsonl(id)\n",
    "                ans = download['answer']\n",
    "                if ans is not None:\n",
    "                    clear_output(wait=True)  # Clear previous output\n",
    "                    total_time_str = str(timedelta(seconds=int(time.time() - start_time)))\n",
    "                    display(HTML(f\"<p>Current time: <b>{current_time}</b></p>\"))\n",
    "                    display(HTML(f\"<p>Total running time: <b>{total_time_str}</b></p>\"))\n",
    "                    display(HTML('<p style=\"color:green;\">Completed!</p>'))  # Display \"Completed\" in green\n",
    "                    return ans\n",
    "                else:\n",
    "                    fullstatus = download['fullstatus']\n",
    "                    completed_count = fullstatus.request_counts.completed\n",
    "                    total_count = fullstatus.request_counts.total\n",
    "                    failed_count = fullstatus.request_counts.failed\n",
    "                    html_content = f\"\"\"\n",
    "                    <p>\n",
    "                        Completed: <b style='color:green'>{completed_count}</b> &nbsp;&nbsp;\n",
    "                        Total: <b style='color:blue'>{total_count}</b> &nbsp;&nbsp;\n",
    "                        Failed: <b style='color:red'>{failed_count}</b>\n",
    "                    </p>\n",
    "                    \"\"\"\n",
    "\n",
    "                    display(HTML(html_content))\n",
    "            except Exception as e:\n",
    "                print(f'Download failed, error: {e}')\n",
    "                \n",
    "        if jobtype == \"embedding\":\n",
    "            try:\n",
    "                import os         \n",
    "                os.environ[\"OPENAI_API_KEY\"] = \"<APIKEYHERE>\"\n",
    "                api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "                api_base = 'https://api.openai.com/v1'\n",
    "                client = OpenAI(api_key=api_key, base_url=api_base)\n",
    "                # Retrieve batch status\n",
    "                full_status = client.batches.retrieve(id)\n",
    "                outputid = full_status.output_file_id\n",
    "                file_response = client.files.content(outputid)\n",
    "                ans_text = file_response.text\n",
    "                list_embedding = [json.loads(line)['response']['body']['data'][0]['embedding'] \n",
    "                  for line in ans_text.split(\"\\n\") if line.strip()]\n",
    "                \n",
    "                clear_output(wait=True)  # Clear previous output\n",
    "                total_time_str = str(timedelta(seconds=int(time.time() - start_time)))\n",
    "                display(HTML(f\"<p>Current time: <b>{current_time}</b></p>\"))\n",
    "                display(HTML(f\"<p>Total running time: <b>{total_time_str}</b></p>\"))\n",
    "                display(HTML('<p style=\"color:green;\">Completed!</p>'))  # Display \"Completed\" in green\n",
    "                    \n",
    "                return list_embedding\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f'Embedding failed: {e}')\n",
    "                \n",
    "        if elapsed_time > max_wait_time_seconds:\n",
    "            clear_output(wait=True)  # Clear previous output\n",
    "            total_time_str = str(timedelta(seconds=int(elapsed_time)))\n",
    "            display(HTML(f\"<p>Current time: <b>{current_time}</b></p>\"))\n",
    "            display(HTML(f\"<p>Total running time: <b>{total_time_str}</b></p>\"))\n",
    "            display(HTML('<p style=\"color:red;\">Exceeded maximum wait time, download failed.</p>'))\n",
    "            return None\n",
    "\n",
    "        if elapsed_time < 600:  # Query every 3 seconds for the first 10 minutes\n",
    "            wait_time = 3\n",
    "        else:  # Query every 1 minute after 10 minutes\n",
    "            wait_time = 60\n",
    "\n",
    "        time.sleep(wait_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9318b82",
   "metadata": {},
   "source": [
    "Extract Secruity-related name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c8c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools\n",
    "\n",
    "def name_extraction_prompt_maker(single_value):\n",
    "    prompts = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": '''Please execute the following steps to extract all proper nouns from the input text:\n",
    "\n",
    "Step 1: Identification\n",
    "\n",
    "    Identify all proper nouns that are specific to the field of computer science.\n",
    "\n",
    "Inclusion Criteria (Rule1):\n",
    "\n",
    "    Only include proper nouns that are directly mentioned in the text.\n",
    "    Focus on specific names related to:\n",
    "        Malware\n",
    "        Unique vulnerabilities (specifically referenced by CVE identifiers)\n",
    "        Distinct network identifiers (IP addresses, domain names, URLs)\n",
    "        Detailed Indicators of Compromise (IOCs) such as file paths, hash values, port numbers, registry key changes, MAC addresses, and unique signatures.\n",
    "        Software names (e.g., antivirus programs, hacking tools, operating systems)\n",
    "        Hardware names (e.g., specific models of routers, servers, or IoT devices)\n",
    "        Specific names of hacker groups or threat actors\n",
    "        Specific names of security protocols or encryption algorithms\n",
    "        Specific names of programming languages, libraries, or frameworks\n",
    "        Specific names of computer science concepts or technologies\n",
    "\n",
    "Step 2: Exclusion\n",
    "    After listing all names according to Rule1, evaluate each against Rule2.\n",
    "    For each name you exclude based on Rule2, provide a concise justification. Rule2 states that you should exclude the following types of proper nouns:\n",
    "        Human names\n",
    "        Country names\n",
    "        City names\n",
    "\n",
    "Final Step: Final Output\n",
    "\n",
    "    After the evaluation, present the final, curated list that adheres to Rule1 and Rule2.\n",
    "\n",
    "    Format request:\n",
    "    You should use the JSON format to present the names in each step.\n",
    "    An example JSON output should look like this. Make sure the keys are the same as \"Step 1\", \"Step 2\", \"Final Step\", and the values are lists of names with double quotes.\n",
    "    {\n",
    "        \"Step 1\": [\"ExampleMalware1\", \"ExampleMalware2\", \"Country1\", \"human1\"],\n",
    "        \"Step 2\": [\"ExampleMalware1:Include\", \"ExampleMalware2:Include\", \"Country1:Exclude,country name\", \"human1:Exclude, human name\"],\n",
    "        \"Final Step\": [\"ExampleMalware1\", \"ExampleMalware2\"]\n",
    "    }\n",
    "            '''\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Understood. I will now proceed to extract the names.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"The malware named DarkComet was used by the hacker group APT29. It exploited the CVE-2012-0158 vulnerability to infiltrate systems using IP address 192.168.0.10. The attack was identified through the SHA256 hash: a3d5f8e7e6b8f2348d7b9d8a6f9a0f2a2c1e1f3f8a9f4f8d6c7f8e8f1f1a3d6b. The affected systems were running the Windows 7 operating system and utilized the D-Link DIR-615 router. The malware communicated using the SSL/TLS protocol and was written in C++. This incident was reported in New York, USA, and highlighted vulnerabilities in network security architecture. The person who discovered it was named John Doe.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": '''```json{\n",
    "                \"Step 1\": [\n",
    "                    \"DarkComet\",\n",
    "                    \"APT29\",\n",
    "                    \"CVE-2012-0158\",\n",
    "                    \"192.168.0.10\",\n",
    "                    \"SHA256:a3d5f8e7e6b8f2348d7b9d8a6f9a0f2a2c1e1f3f8a9f4f8d6c7f8e8f1f1a3d6b\",\n",
    "                    \"Windows 7\",\n",
    "                    \"D-Link DIR-615\",\n",
    "                    \"SSL/TLS\",\n",
    "                    \"C++\",\n",
    "                    \"New York\",\n",
    "                    \"USA\",\n",
    "                    \"John Doe\",\n",
    "                    \"network security architecture\"\n",
    "                ],\n",
    "                \"Step 2\": [\n",
    "                    \"DarkComet:Include\",\n",
    "                    \"APT29:Include\",\n",
    "                    \"CVE-2012-0158:Include\",\n",
    "                    \"192.168.0.10:Include\",\n",
    "                    \"SHA256:a3d5f8e7e6b8f2348d7b9d8a6f9a0f2a2c1e1f3f8a9f4f8d6c7f8e8f1f1a3d6b:Include\",\n",
    "                    \"Windows 7:Include\",\n",
    "                    \"D-Link DIR-615:Include\",\n",
    "                    \"SSL/TLS:Include\",\n",
    "                    \"C++:Include\",\n",
    "                    \"New York:Exclude, city name\",\n",
    "                    \"USA:Exclude, country name\",\n",
    "                    \"John Doe:Exclude, human name\",\n",
    "                    \"network security architecture:Include\"\n",
    "                ],\n",
    "                \"Final Step\": [\n",
    "                    \"DarkComet\",\n",
    "                    \"APT29\",\n",
    "                    \"CVE-2012-0158\",\n",
    "                    \"192.168.0.10\",\n",
    "                    \"SHA256:a3d5f8e7e6b8f2348d7b9d8a6f9a0f2a2c1e1f3f8a9f4f8d6c7f8e8f1f1a3d6b\",\n",
    "                    \"Windows 7\",\n",
    "                    \"D-Link DIR-615\",\n",
    "                    \"SSL/TLS\",\n",
    "                    \"C++\",\n",
    "                    \"network security architecture\"\n",
    "                ]\n",
    "            }```'''\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"The hacker group known as Fancy Bear used the malware Zeus to exploit the CVE-2015-2426 vulnerability. The attack was carried out via the domain name malicious-site.com and was tracked using the IOC MD5 hash: 5d41402abc4b2a76b9719d911017c592. The targeted systems were protected by the McAfee antivirus and operated on the Ubuntu Linux OS, connected through a Netgear Nighthawk X6 router. Communications were secured by the SSH protocol, and the attack code was written in Python. The incident was detected in Tokyo, Japan, and the initial analysis was done by Jane Smith.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": '''```json{\n",
    "                \"Step 1\": [\n",
    "                    \"Fancy Bear\",\n",
    "                    \"Zeus\",\n",
    "                    \"CVE-2015-2426\",\n",
    "                    \"malicious-site.com\",\n",
    "                    \"MD5:5d41402abc4b2a76b9719d911017c592\",\n",
    "                    \"McAfee\",\n",
    "                    \"Ubuntu Linux\",\n",
    "                    \"Netgear Nighthawk X6\",\n",
    "                    \"SSH\",\n",
    "                    \"Python\",\n",
    "                    \"Tokyo\",\n",
    "                    \"Japan\",\n",
    "                    \"Jane Smith\"\n",
    "                ],\n",
    "                \"Step 2\": [\n",
    "                    \"Fancy Bear:Include\",\n",
    "                    \"Zeus:Include\",\n",
    "                    \"CVE-2015-2426:Include\",\n",
    "                    \"malicious-site.com:Include\",\n",
    "                    \"MD5:5d41402abc4b2a76b9719d911017c592:Include\",\n",
    "                    \"McAfee:Include\",\n",
    "                    \"Ubuntu Linux:Include\",\n",
    "                    \"Netgear Nighthawk X6:Include\",\n",
    "                    \"SSH:Include\",\n",
    "                    \"Python:Include\",\n",
    "                    \"Tokyo:Exclude, city name\",\n",
    "                    \"Japan:Exclude, country name\",\n",
    "                    \"Jane Smith:Exclude, human name\"\n",
    "                ],\n",
    "                \"Final Step\": [\n",
    "                    \"Fancy Bear\",\n",
    "                    \"Zeus\",\n",
    "                    \"CVE-2015-2426\",\n",
    "                    \"malicious-site.com\",\n",
    "                    \"MD5:5d41402abc4b2a76b9719d911017c592\",\n",
    "                    \"McAfee\",\n",
    "                    \"Ubuntu Linux\",\n",
    "                    \"Netgear Nighthawk X6\",\n",
    "                    \"SSH\",\n",
    "                    \"Python\"\n",
    "                ]\n",
    "            }```'''\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"A notorious hacker group called Anonymous deployed the Stuxnet malware by exploiting CVE-2010-2568. The malware was spread through the IP address 10.0.0.1 and was identified by the SHA1 hash: da39a3ee5e6b4b0d3255bfef95601890afd80709. The infected systems were running the Debian OS and were protected by Symantec Antivirus, connected through a Cisco ASA 5505 firewall. The malware communication was encrypted using the RSA algorithm, and the attack script was developed using the Java programming language. This attack was reported in Berlin, Germany, by a cybersecurity analyst named Michael Johnson.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": '''```json{\n",
    "                \"Step 1\": [\n",
    "                    \"Anonymous\",\n",
    "                    \"Stuxnet\",\n",
    "                    \"CVE-2010-2568\",\n",
    "                    \"10.0.0.1\",\n",
    "                    \"SHA1:da39a3ee5e6b4b0d3255bfef95601890afd80709\",\n",
    "                    \"Debian\",\n",
    "                    \"Symantec Antivirus\",\n",
    "                    \"Cisco ASA 5505\",\n",
    "                    \"RSA\",\n",
    "                    \"Java\",\n",
    "                    \"Berlin\",\n",
    "                    \"Germany\",\n",
    "                    \"Michael Johnson\"\n",
    "                ],\n",
    "                \"Step 2\": [\n",
    "                    \"Anonymous:Include\",\n",
    "                    \"Stuxnet:Include\",\n",
    "                    \"CVE-2010-2568:Include\",\n",
    "                    \"10.0.0.1:Include\",\n",
    "                    \"SHA1:da39a3ee5e6b4b0d3255bfef95601890afd80709:Include\",\n",
    "                    \"Debian:Include\",\n",
    "                    \"Symantec Antivirus:Include\",\n",
    "                    \"Cisco ASA 5505:Include\",\n",
    "                    \"RSA:Include\",\n",
    "                    \"Java:Include\",\n",
    "                    \"Berlin:Exclude, city name\",\n",
    "                    \"Germany:Exclude, country name\",\n",
    "                    \"Michael Johnson:Exclude, human name\"\n",
    "                ],\n",
    "                \"Final Step\": [\n",
    "                    \"Anonymous\",\n",
    "                    \"Stuxnet\",\n",
    "                    \"CVE-2010-2568\",\n",
    "                    \"10.0.0.1\",\n",
    "                    \"SHA1:da39a3ee5e6b4b0d3255bfef95601890afd80709\",\n",
    "                    \"Debian\",\n",
    "                    \"Symantec Antivirus\",\n",
    "                    \"Cisco ASA 5505\",\n",
    "                    \"RSA\",\n",
    "                    \"Java\"\n",
    "                ]\n",
    "            }```'''\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Good job! Now, I will give you another new sentence. Follow the same steps to extract the names and provide the JSON format output. Don't use the example names from the previous example. Here is the sentence: \\\"{single_value}\\\"\"\n",
    "        }\n",
    "    ]\n",
    "    return prompts\n",
    "\n",
    "input_list=Core_DataFrame[\"content part\"].tolist()\n",
    "input_prompts_batch=[]\n",
    "for i in input_list:\n",
    "    input_prompts_batch.append(name_extraction_prompt_maker(i))\n",
    "input_prompts_batch=input_prompts_batch*3\n",
    "\n",
    "code_name=\"SECCG CVE website process name extraction\"\n",
    "\n",
    "jsonl_file= tools.create_jsonl(input_prompts_batch,model='gpt4', temp=1, token=16384, jsonlname=code_name, possible_output=300)\n",
    "    \n",
    "ids = tools.upload_RUN_PAY_jsonl(jsonl_file, code_name)\n",
    "\n",
    "ans=tools.auto_down_ans(ids['batch_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70222ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tools.ans_to_df(ans)\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "all_keep_items = []\n",
    "total_correct = 0\n",
    "partial_nan_count = 0\n",
    "all_nan_count = 0\n",
    "\n",
    "for i in range(len(input_list)):\n",
    "    try:\n",
    "        step1 = df.loc[i, 'Final Step']\n",
    "        step2 = df.loc[i + len(input_list), 'Final Step']\n",
    "        step3 = df.loc[i + 2 * len(input_list), 'Final Step']\n",
    "\n",
    "        step1_is_nan = isinstance(step1, float) and np.isnan(step1)\n",
    "        step2_is_nan = isinstance(step2, float) and np.isnan(step2)\n",
    "        step3_is_nan = isinstance(step3, float) and np.isnan(step3)\n",
    "\n",
    "        nan_count = sum([step1_is_nan, step2_is_nan, step3_is_nan])\n",
    "\n",
    "        if nan_count == 0:\n",
    "            total_correct += 1\n",
    "        elif nan_count == 3:\n",
    "            all_nan_count += 1\n",
    "        else:\n",
    "            partial_nan_count += 1\n",
    "\n",
    "        step1 = step1 if isinstance(step1, list) else []\n",
    "        step2 = step2 if isinstance(step2, list) else []\n",
    "        step3 = step3 if isinstance(step3, list) else []\n",
    "\n",
    "        combined_steps = step1 + step2 + step3\n",
    "        keep_items = [item for item, count in Counter(combined_steps).items() if count >= 2]\n",
    "        all_keep_items.append(keep_items)\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {i}: {e}\")\n",
    "        all_keep_items.append([])\n",
    "\n",
    "Core_DataFrame['specific_names'] = all_keep_items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e210904",
   "metadata": {},
   "source": [
    "Classify CTI sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d50dfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize # type: ignore\n",
    "import pickle\n",
    "all_sentences = [sentence for i in range(Core_DataFrame.shape[0]) for sentence in sent_tokenize(Core_DataFrame.loc[i, 'content part'])]\n",
    "\n",
    "user_input = input('About to save pkl for colab usage, input \"Y\", \"y\" or \"1\" to continue: ').lower()\n",
    "\n",
    "# Check if the input is \"y\", \"1\", or \"Y\"\n",
    "if user_input in ['y', '1']:\n",
    "    # Remove the previous file\n",
    "    try:\n",
    "        os.remove(\"transferColab_sentences_list.pkl\")\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    # Save the new pkl file\n",
    "    with open(\"transferColab_sentences_list.pkl\", \"wb\") as file:\n",
    "        pickle.dump(all_sentences, file)\n",
    "    print(\"Saved as transferColab_sentences_list.pkl. The size is:\", len(all_sentences))\n",
    "else:\n",
    "    print(\"Operation canceled.\")\n",
    "    \n",
    "# Read back the sentence classification file ↑ >>>\n",
    "print('Reading: download_back_sentence classification_simple.pkl file from colab')\n",
    "with open('download_back_sentence_classification_simple.pkl', 'rb') as file:\n",
    "    sentence_label_dict = pickle.load(file)\n",
    "print(\"Output length:\", len(sentence_label_dict))\n",
    "sentence_CTIlabel_dict = {key: value for key, value in sentence_label_dict.items() if '1' in str(value)}\n",
    "\n",
    "sentence_CTIlabel_key = list(sentence_CTIlabel_dict.keys())\n",
    "\n",
    "Core_DataFrame['CTIlabelsentence_inpart'] = ''\n",
    "for index in range(Core_DataFrame.shape[0]):\n",
    "    content_part = Core_DataFrame.loc[index, 'content part']\n",
    "    CTI_sentence = []\n",
    "    for single_sentence in sentence_CTIlabel_key:\n",
    "        if single_sentence in content_part:\n",
    "            CTI_sentence.append(single_sentence + '\\n')\n",
    "    \n",
    "    Core_DataFrame.loc[index, 'CTIlabelsentence_inpart'] = ''.join(CTI_sentence)\n",
    "print('Added: CTIlabelsentence_inpart column, to indicate CTI sentences in the part')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61523c3e",
   "metadata": {},
   "source": [
    "Worker Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9d6c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(text,extra_prompt,extra_prompt_hightlight,extra_prompt_targetname):\n",
    "    promptmessage = [\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \n",
    "    ''' \n",
    "    As an AI trained in entity extraction and relationship extraction. You're an advanced AI expert, so even if I give you a complex sentence, you'll still be able to perform the relationship extraction task. The ouput of the task is a list of list.\n",
    "    \n",
    "    Triple format explanation:\n",
    "        The triple is a basic data structure used to represent knowledge in the form of a semantic graph. It consists of three elements:\n",
    "\n",
    "        Subject: A noun that represents the main entity or concept.\n",
    "        Relation: A verb or phrase that describes the relationship between the subject and the object.\n",
    "        Object: A noun that represents the entity or concept related to the subject through the relation.\n",
    "        \n",
    "        Example of a triple: [Formbook, is, malware]. \"Formbook\" is the subject, \"is\" is the relation, and \"malware\" is the object. This structure helps in describing how entities are related within a specific context, such as computer science.\n",
    "        \n",
    "        As counterexample: [FinSpy malware, was the final payload] is not a valid triple, sicne it does not contain exactly 3 elements. Also, [FinSpy malware, was, the final payload, that will be used] is not a valid triple, since it contains 4 elements. The subject and the object should be Noun. The relation should be a relation words that connects the subject and the object, and expresses how they are related.\n",
    "        \n",
    "    Your output format requirement:\n",
    "        \"The expected output format is json format. It has key as \"worker agent\" and value as a list of list.\"\n",
    "    \n",
    "    You complete the task by following the rules below:\n",
    "    \n",
    "        Rule 1:\n",
    "        You extract triples related to computer science, focusing on sentences explicitly involving concepts and entities related to the field, such as programming languages, software and hardware technologies, algorithms, network protocols, security vulnerabilities, operating systems, etc.\n",
    "\n",
    "        Example for Rule 1:\n",
    "        Input: \"Formbook is malware, while the Sun is a star and Picasso is a painter.\"\n",
    "        Output: {\"worker agent\": [[\"Formbook\", \"is\", \"malware\"]]}\n",
    "        Explanation: The triple [\"Formbook\", \"is\", \"malware\"] is related to computer science as it describes a relationship involving a malware. The other triples, [\"Sun\", \"is\", \"star\"] and [\"Picasso\", \"is\", \"painter\"], are excluded because they pertain to astronomy and art, not computer science.\n",
    "\n",
    "        Rule 2:\n",
    "        If the input sentences do not contain any computer science-related triples, you must output: [[Blank placeholders since the current text does not contain any computer science-related triples]].\n",
    "\n",
    "        Example for Rule 2:\n",
    "        Input: \"The Sun is a star and Picasso is a painter.\"\n",
    "        Output: {\"worker agent\": [[\"Blank placeholders since the current text does not contain any computer science-related triples\"]]}\n",
    "        Explanation: The input sentence does not contain any triples related to computer science, so the output is a blank placeholder.\n",
    "\n",
    "        Rule 3:\n",
    "        If you find that you are keeping output triples not related to computer science, stop as soon as possible and output: [[Blank placeholders since the current text does not contain any computer science-related triples]].\n",
    "\n",
    "        Example for Rule 3:\n",
    "        Input: \"The Sun is a star and Picasso is a painter. Jack likes Picasso's painting.\"\n",
    "        Output: {\"worker agent\": [[\"Blank placeholders since the current text does not contain any computer science-related triples\"]]}\n",
    "        Explanation: Although you might identify triples initially, none of them are related to computer science. Therefore, you should stop and output the blank placeholder, overriding any previously considered triples.\n",
    "\n",
    "        Rule 4:\n",
    "        The subject or object should be strictly singular. Remove all non-computer-related adjectives. Verbs must be in the infinitive form, excluding plural and any non-simple tense.\n",
    "\n",
    "        Example for Rule 4:\n",
    "        Input: \"Formbook is a very dangerous malware, Formbook checked the system.\"\n",
    "        Output: {\"worker agent\": [[\"Formbook\", \"be\", \"malware\"], [\"Formbook\", \"check\", \"system\"]]}\n",
    "        Explanation: The subject \"Formbook\" is singular. The adjective \"very dangerous\" is removed because it's not computer-related, and the verb \"checked\" is converted to its infinitive form \"check\".\n",
    "\n",
    "        Rule 5:\n",
    "        If the subject or object in a triple contains pronouns such as \"it,\" \"they,\" \"malware,\" \"Trojan,\" \"attack,\" \"ransomware,\" or other general terms, replace them with the specific name they refer to in the sentence.\n",
    "\n",
    "        Example for Rule 5:\n",
    "        Input: \"The malware is Formbook. This malware is used to steal sensitive information. It is a banking malware.\"\n",
    "        Output: {\"worker agent\": [[\"Formbook\", \"is\", \"malware\"], [\"Formbook\", \"is used to steal\", \"sensitive information\"], [\"Formbook\", \"is\", \"banking malware\"]]}\n",
    "        Explanation: The pronouns \"this malware\" and \"it\" refer to \"Formbook,\" so they are replaced accordingly.\n",
    "\n",
    "        Rule 6:\n",
    "        If the subject or object in a triple contains multiple entities with parallel relationships to the same object or subject, split these entities into separate triples.\n",
    "\n",
    "        Example for Rule 6:\n",
    "        Input: \"Formbook and Trickbot are malware. They are used to steal sensitive information.\"\n",
    "        Output: {\"worker agent\": [[\"Formbook\", \"is\", \"malware\"], [\"Trickbot\", \"is\", \"malware\"], [\"Formbook\", \"is used to steal\", \"sensitive information\"], [\"Trickbot\", \"is used to steal\", \"sensitive information\"]]}\n",
    "        Explanation: The sentence contains two entities, \"Formbook\" and \"Trickbot,\" that relate to \"malware\" and the action of \"stealing sensitive information.\" These are split into separate triples to accurately represent their relationships.\n",
    "\n",
    "        Rule 7:\n",
    "        For entities described by relative pronouns, extract triples based on the relative pronouns.\n",
    "\n",
    "        Example for Rule 7:\n",
    "        Input: \"The malware, which is known as Formbook, is used to steal sensitive information when it is downloaded from the internet.\"\n",
    "        Output: {\"worker agent\": [[\"Formbook\", \"is\", \"malware\"], [\"Formbook\", \"is used to steal\", \"sensitive information\"], [\"Formbook\", \"downloaded from\", \"internet\"]]}\n",
    "        Explanation: The relative pronoun \"which\" refers to \"Formbook,\" so the triples are extracted based on this relationship.\n",
    "\n",
    "        Rule 8:\n",
    "        For entities that have a probabilistic relationship (e.g., \"could,\" \"can,\" \"may\"), extract the triples as if the relationship is true.\n",
    "\n",
    "        Example for Rule 8:\n",
    "        Input: \"Malware like Formbook could be used to steal sensitive information, and when downloaded from the internet, it may cause data loss.\"\n",
    "        Output: {\"worker agent\": [[\"Formbook\", \"could be used to steal\", \"sensitive information\"], [\"Formbook\", \"downloaded from\", \"internet\"], [\"Formbook\", \"may cause\", \"data loss\"]]}\n",
    "        Explanation: The input sentence suggests a probabilistic relationship, so the triples are extracted as though these relationships are true.\n",
    "\n",
    "        Rule 9:\n",
    "        The relationship between the subject and object can be based on \"of.\" In this case, \"of\" is the relationship word.\n",
    "\n",
    "        Example for Rule 9:\n",
    "        Input: \"The analysis of the malware Formbook shows that it is used to steal sensitive information.\"\n",
    "        Output: {\"worker agent\": [[\"analysis\", \"of\", \"malware Formbook\"], [\"analysis\", \"shows\", \"it is used to steal sensitive information\"]]}\n",
    "        Explanation: The word \"of\" is used as the relationship between \"analysis\" and \"malware Formbook,\" creating the first triple.\n",
    "\n",
    "        Rule 10:\n",
    "        Consider gerunds as verbs in the relationship. Convert the gerund to the verb and output two triples: one where the gerund acts as the relation, and another where it acts as the object.\n",
    "\n",
    "        Example for Rule 10:\n",
    "        Input: \"The most common way of Formbook stealing sensitive information is through phishing emails.\"\n",
    "        Output: {\"worker agent\": [[\"Formbook\", \"steal\", \"sensitive information\"], [\"way of Formbook stealing sensitive information\", \"is through\", \"phishing emails\"]]}\n",
    "        Explanation: The gerund \"stealing\" is treated as a verb in the triple \"Formbook steal sensitive information.\" Additionally, the phrase \"way of Formbook stealing sensitive information\" is used as a subject in another triple.\n",
    "\n",
    "        Rule 11:\n",
    "        The object in the triple should be a noun without any verb or adjective. If the object has a verb or adjective, split the object into a new triple.\n",
    "\n",
    "        Example for Rule 11:\n",
    "        Input: \"Formbook is a malware designed to run as a deleter.\"\n",
    "        Output: {\"worker agent\": [[\"Formbook\", \"is\", \"malware\"], [\"malware\", \"designed to run as\", \"deleter\"]]}\n",
    "        Explanation: The object \"malware\" has a descriptive verb phrase \"designed to run as,\" which is split into a separate triple.\n",
    "\n",
    "        Rule 12:\n",
    "        If the relationship between the subject and object contains a descriptive noun, output two triples: one with the full relationship and another with the descriptive noun as the subject.\n",
    "\n",
    "        Example for Rule 12:\n",
    "        Input: \"Formbook dumps information of low-level system settings to a text file.\"\n",
    "        Output: {\"worker agent\": [[\"Formbook\", \"dumps\", \"information\"], [\"information\", \"of\", \"low-level system settings\"]]}\n",
    "        Explanation: The descriptive noun \"information\" can be used as the subject in a new triple, focusing on its relationship with \"low-level system settings.\"\n",
    "              \n",
    "    \\\n",
    "    '''\n",
    "    },\n",
    "    {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"I got it.\"\n",
    "    },\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Here is one sentence from example article:\\\"Leafminer attempts to infiltrate target networks through various means of intrusion: watering hole websites, vulnerability scans of network services on the internet, and brute-force/dictionary login attempts.\\\"\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Here is one sentence from example article:\\\"Kismet is also a powerful tool for penetration testers that need to better understand their target and perform wireless LAN discovery.\\\"\"\n",
    "    },\n",
    "    {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \n",
    "        \"\"\"```{\n",
    "        \"worker agent\": [\n",
    "            [\"Kismet\", \"is\", \"tool\"],\n",
    "            [\"tool\", \"for\", \"penetration testers\"],\n",
    "            [\"penetration testers\", \"need to better understand\", \"target\"],\n",
    "            [\"penetration testers\", \"perform\", \"wireless LAN discovery\"]\n",
    "        ]\n",
    "    }```\n",
    "    \"\"\"\n",
    "    },\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Here is one sentence from example article:\\\"Legendary Pokémon , or Pokémon Illusions are extremely rare and often very powerful Pokémon that are often associated with legends of creation and/or destruction within their endemic regions. \\\"\"\n",
    "    },\n",
    "    {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"```{\\\"worker agent\\\": [[\\\"Blank placeholders since the current text does not contain any computer science related triples\\\"]]}```\"\n",
    "    },\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Here is one sentence from example article:\\\"The Royal Knights  are a group of thirteen Mega-level[1] Holy Warrior Digimon[2] that are the Digital World's sacred guardians,[3] and are famed among Digimon as guardian deities of the Computer Network.[4][5] The group was founded by Imperialdramon Paladin Mode,\\\"\"\n",
    "    },\n",
    "    {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"```{\\\"worker agent\\\": [[\\\"The Royal Knights\\\", \\\"are\\\", \\\"a group of thirteen Mega-level Holy Warrior Digimon\\\"], [\\\"The Royal Knights\\\", \\\"are\\\", \\\"the Digital World's sacred guardians\\\"], [\\\"The Royal Knights\\\", \\\"are\\\", \\\"guardian deities of the Computer Network\\\"], [\\\"The group\\\", \\\"was founded by\\\", \\\"Imperialdramon Paladin Mode\\\"], [\\\"I find that the sentence does not contain any computer science related triples, and I am keeping output triples that are not related to computer science. So I stop here and output Blank placeholders in the end since the current text does not contain any computer science related triples\\\", [\\\"Blank placeholders since the current text does not contain any computer science related triples\\\"]]}```\"\n",
    "    },\n",
    "    {\"role\": \"user\",\n",
    "    \"content\": \n",
    "    \"\"\"\n",
    "    Here are my new sentence, extract all possible entity triples from it. Now, I start to give you sentence.\\\"\"\n",
    "    \"\"\"+text+\n",
    "    \"\"\"\\\"Now, my input text are over. You MUST follow the rules I told you. \n",
    "    \"\"\"+extra_prompt+extra_prompt_hightlight+extra_prompt_targetname\n",
    "    },\n",
    "    ]\n",
    "    return promptmessage\n",
    "\n",
    "all_Worker_prompts=[]\n",
    "\n",
    "for index in range(Core_DataFrame.shape[0]):\n",
    "    enhance=True\n",
    "    extra_highlight_word=\"\"\n",
    "    extra_highlight_sentence=\"\"\n",
    "    keywords=Core_DataFrame.loc[index, 'specific_names']\n",
    "    if keywords!=None and len(keywords)>0 and enhance == True:\n",
    "        extra_highlight_word = \"\\nSupport information for this task: When you find those keywords in above sentence, you should pay more attention to them and extract more triples about them:\\\"\"+str(keywords)+'\\\"'\n",
    "\n",
    "    hightlight=Core_DataFrame.loc[index, 'CTIlabelsentence_inpart']    \n",
    "    if hightlight!=None and len(hightlight)>0 and enhance == True:\n",
    "        extra_highlight_sentence=\"\\nSupport information for this task: those sentences contain some important information, you should pay more attention to them and extract more triples from them.\\\"\"+str(hightlight)+'\\\"'\n",
    "    \n",
    "    text_thispart=Core_DataFrame.loc[index, 'content part']\n",
    "    \n",
    "    Target_name=Core_DataFrame.loc[index, 'CVE True Name']\n",
    "    \n",
    "    extra_prompt_targetname=\"\\n Support information for this task: In my input, if you find some text are discussing about a specific CVE's information, but the CVE name is missing, you should consider the Key CVE name as the subject name, and output triple with that Key CVE name as the subject name. For example, if you find the example sentence like:\\\"Vulnerability's target: Formbook, Kismet, and Leafminer\\\" with the example Key CVE name as CVE-2000-1234, you should consider Key CVE name as the subject name, and output triple with that Key CVE name as the subject name, like this:```{worker agent: [[\\\"CVE-2000-1234\\\", \\\"target\\\", \\\"Formbook\\\"], [\\\"CVE-2000-1234\\\", \\\"target\\\", \\\"Kismet\\\"], [\\\"CVE-2000-1234\\\", \\\"target\\\", \\\"Leafminer\\\"]]}```. Now, the actual key CVE name of my input text is:\\\"\"+str(Target_name)+'\\\"'\n",
    "\n",
    "    promptmessage=generate_prompt(text_thispart,extra_highlight_word,extra_highlight_sentence,extra_prompt_targetname)\n",
    "    #promptmessage=generate_prompt(text_thispart,\"\",\"\",\"\")\n",
    "    \n",
    "    all_Worker_prompts.append(promptmessage)\n",
    "    \n",
    "all_prompts_Worker_3times=all_Worker_prompts*3\n",
    "\n",
    "code_name=\"SECCG worker agent V2 mini\"\n",
    "jsonl_file= tools.create_jsonl(all_prompts_Worker_3times,model='gpt4mini', temp=1, token=16384, jsonlname=code_name, possible_output=300)\n",
    "\n",
    "ids = tools.upload_RUN_PAY_jsonl(jsonl_file, code_name)\n",
    "\n",
    "ans=tools.auto_down_ans(ids['batch_id'])\n",
    "\n",
    "df=tools.ans_to_df(ans)\n",
    "\n",
    "if len(ans)/3==len(all_Worker_prompts):\n",
    "    Core_DataFrame['worker1_result']=''\n",
    "    Core_DataFrame['worker2_result']=''\n",
    "    Core_DataFrame['worker3_result']=''\n",
    "    Core_DataFrame['blank_content_flag']=''\n",
    "    \n",
    "\n",
    "    for index in range(len(all_Worker_prompts)):\n",
    "        agent_1_result=str(df.loc[index, 'worker agent'])\n",
    "        agent_2_result=str(df.loc[index+len(all_Worker_prompts),'worker agent'])\n",
    "        agent_3_result=str(df.loc[index+2*len(all_Worker_prompts),'worker agent'])\n",
    "        \n",
    "        for keyword in ['CVExxx', 'Formbook', 'XLoader', 'Malwaresavetextfile', 'Leafminer', 'FinSpy', 'Kismet', 'Specificnamesofa']:\n",
    "            if keyword in agent_1_result and keyword not in Core_DataFrame.loc[index, 'content part']:\n",
    "                agent_1_result=\"ERROR\"\n",
    "            if keyword in agent_2_result and keyword not in Core_DataFrame.loc[index, 'content part']:\n",
    "                agent_2_result=\"ERROR\"\n",
    "            if keyword in agent_3_result and keyword not in Core_DataFrame.loc[index, 'content part']:\n",
    "                agent_3_result=\"ERROR\"\n",
    "        \n",
    "        blank_flag_count=0\n",
    "        if \"Blank placeholders\" in agent_1_result:\n",
    "            blank_flag_count+=1\n",
    "        if \"Blank placeholders\" in agent_2_result:\n",
    "            blank_flag_count+=1\n",
    "        if \"Blank placeholders\" in agent_3_result:\n",
    "            blank_flag_count+=1\n",
    "            \n",
    "        if blank_flag_count>=2:\n",
    "            Core_DataFrame.loc[index, 'blank_content_flag'] = True\n",
    "        else:\n",
    "            Core_DataFrame.loc[index, 'blank_content_flag'] = False\n",
    "            \n",
    "        Core_DataFrame.loc[index, 'worker1_result'] = str(agent_1_result)\n",
    "        Core_DataFrame.loc[index, 'worker2_result'] = str(agent_2_result)\n",
    "        Core_DataFrame.loc[index, 'worker3_result'] = str(agent_3_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba58994c",
   "metadata": {},
   "source": [
    "Integrator Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ab85b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_Integrator(inlist_A, inlist_B, inlist_C):\n",
    "    promptmessage = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"Your primary objective is to consolidate entity extraction results derived from sentence or sentences by different distinct assistants into a singular, coherent output. This process necessitates a careful analysis to identify and merge triples that, despite potentially differing in phrasing, communicate identical information. Additionally, it is imperative to filter out any triples that fail to meet the specified criteria for validity and relevance.\n",
    "\n",
    "            Key Points: \n",
    "            1. Eliminate Invalid Triples: Any triples not conforming to the structure of having exactly one SUBJECT, one RELATION, and one OBJECT should be removed. \n",
    "            2. Exclude triples missing any of the three essential components [SUBJECT, RELATION, OBJECT]. \n",
    "            3. Discard any triples that introduce information not explicitly mentioned in the provided sentence. \n",
    "            4. Consolidation means merge triples that convey the same information but are expressed using different terminologies or structures. This involves combining synonyms or semantically similar phrases that refer to the same entities or actions.\n",
    "            \n",
    "            Your answer should be in JSON format. The keys are \"My thoughts for this task\" and \"Final Output\". You should first output your thoughts for this task in the \"My thoughts for this task\" key, and then output the final output in the \"Final Output\" key. The value of \"My thoughts for this task\" is a long string, and the value of \"Final Output\" is a list of lists.\"\"\"\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": \"Understood.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"My inputs are:\n",
    "            Assistant A: [FinSpy, designed by, Gamma Group], [FinSpy, to spy on, digital communications], [FinSpy, is named by, Gamma Group]\n",
    "            Assistant B: [FinSpy, is designed by, Gamma Group], [FinSpy, is designed from, Gamma Group], [Surveillance software, is used for, spying on digital communications]\n",
    "            Assistant C: [FinSpy, developed by, Gamma Group], [FinSpy, be said that is designed by, Gamma Group], [FinSpy, monitors, communications],[FinSpy, has]\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"\"\"\n",
    "            ```{\n",
    "                \"My thoughts for this task\": \"First, I will identify and remove any redundant triples across the inputs. For example, 'designed by' and 'developed by' are similar, but 'designed by' is more precise, so I will retain 'designed by' as the final relation. Next, I will consolidate triples that express the same idea in different words, such as 'spy on' and 'monitors,' prioritizing the more frequently used term, which in this case is 'spy on.' I will then discard any duplicate triples that convey the same information, such as '[FinSpy, is designed by, Gamma Group]' and '[FinSpy, is designed from, Gamma Group],' keeping only one instance. Next, I will remove any triples that appear in only one assistant's output, like '[FinSpy, is named by, Gamma Group].' Next, I will discard any triples that do not have exactly three elements, such as '[FinSpy, has],' which only has two. Finally, I will output the final result in the 'Final Output' key.\",\n",
    "                \"Final Output\": [\n",
    "                    [\"FinSpy\", \"designed by\", \"Gamma Group\"],\n",
    "                    [\"FinSpy\", \"to spy on\", \"digital communications\"]\n",
    "                ]\n",
    "            }```\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Good, this time I will give you a new Original Sentence and three assistants' results. Follow the same steps to consolidate the results and provide the JSON format output. Here is the new three Assistant results:\\\"\"\n",
    "            + \"Assistant A: \" + str(inlist_A)\n",
    "            + \"Assistant B: \" + str(inlist_B)\n",
    "            + \"Assistant C: \" + str(inlist_C)\n",
    "            + \"Now, my inputs are end.\"\n",
    "        }\n",
    "    ]\n",
    "    return promptmessage\n",
    "\n",
    "all_prompts_Integrator=[]\n",
    "for index in range(Core_DataFrame.shape[0]):\n",
    "    text_thispart=Core_DataFrame.loc[index, 'content part']\n",
    "    worker1_result=Core_DataFrame.loc[index, 'worker1_result']\n",
    "    worker2_result=Core_DataFrame.loc[index, 'worker2_result']\n",
    "    worker3_result=Core_DataFrame.loc[index, 'worker3_result']\n",
    "    \n",
    "    promptmessage=generate_prompt_Integrator(worker1_result,worker2_result,worker3_result)\n",
    "    \n",
    "    all_prompts_Integrator.append(promptmessage)\n",
    "\n",
    "code_name=\"SECCG Integrator agent\"\n",
    "jsonl_file= tools.create_jsonl(all_prompts_Integrator,model='gpt4', temp=1, token=16384, jsonlname=code_name, possible_output=300)\n",
    "\n",
    "\n",
    "ids = tools.upload_RUN_PAY_jsonl(jsonl_file, code_name)\n",
    "\n",
    "ans=tools.auto_down_ans('batch_uR8lyeieivKcwTFrAOTHg3mk')\n",
    "\n",
    "total_results = [\n",
    "    f\"WORKER 1 result: {row['worker1_result']}\\n\"\n",
    "    f\"WORKER 2 result: {row['worker2_result']}\\n\"\n",
    "    f\"WORKER 3 result: {row['worker3_result']}\"\n",
    "    for _, row in Core_DataFrame.iterrows()\n",
    "]\n",
    "\n",
    "df_withIntegrator=tools.ans_and_inputs_to_df([total_results],['SECCG 3 Worker Result'],all_prompts_Integrator,ans)\n",
    "\n",
    "Core_DataFrame['Integrator_result']=df_withIntegrator['Final Output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30467979",
   "metadata": {},
   "source": [
    "Refiner Agnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4095d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_postprocess(text):\n",
    "        promptmessage = [\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \n",
    "        ''' \n",
    "        You play the role of an entity extraction expert and modify/simplify/split the text (extracted multiple triples) in the entity extraction result I gave you (a python dictionary with key as the source sentence with ellipsis and value as the extracted triples) according to the following rules. A triple is a basic data structure used to represent knowledge graphs, which are structured semantic knowledge bases that describe concepts and their relationships in the physical world. A triple consists of three elements: [SUBJECT, RELATION,OBJECT]. The subject and the object are entities, which can be things, people, places, events, or abstract concepts. The relation is a relation that connects the subject and the object, and expresses how they are related. \n",
    "        \n",
    "        For example, [Formbook, is, malware] is a triple that describes the relationship between the malware Formbook and the concept of malware.\n",
    "        \n",
    "        You should follow the rules below to modify the triples:\n",
    "        RULE 1: Simplify the subject, object, and relation into a more concise, generic expression. When you encounter a plural or past tense form, convert it to singular or present tense.  \n",
    "        Example for Rule 1: Input: \"[Formbook, targets, Windows users],\". Output:```json {\"Thoughts\": \"The word \"targets\" is in plural form, so I converted it to singular form.\", \"Triple\": \"[Formbook, target, Windows user]\"} ```\n",
    "        \n",
    "        RULE2: If a subject or object contains a proper noun that is a specific name of a malware, Trojan horse, CVE, or hacking organization, remove the proper noun and keep the generic term. When you encounter such subject or object that contains modifiers and adjectives, remove the modifiers and adjectives to keep the core term.\n",
    "        Example for Rule 2: Input: \"[Formbook malware, use, website bookmarks]\". Output:```json {\"Thoughts\": \"The word \"Formbook\" is a specific name of a malware, so I removed the additional suffix, which is \"malware\".\", \"Triple\": \"[Formbook, use, website bookmark]\"} ```\n",
    "        \n",
    "        Rule3: If the object itself contains a new relation, create a new triple based on the object, and convert the original triple into a simple form.\n",
    "        Example for Rule 3: Input: \"[Formbook, save dump file to, a folder in desktop]\". Output:```json {\"Thoughts\": \"The object \"a folder in desktop\" contains a new relation \"in desktop\", so I created a new triple based on the object.\", \"Triple1\": \"[Formbook, save dump file, a folder]\", \"Triple2\": \"[a folder, in, desktop]\"} ```\n",
    "        \n",
    "        Rule 4: Split a complex triple into multiple simpler forms. \n",
    "        Example for Rule 4: Input: \"[Formbook and XLoader, are, malware]\". Output:```json {\"Thoughts\": \"The triple contains two subjects, so I split it into two triples.\", \"Triple1\": \"[Formbook, be, malware]\", \"Triple2\": \"[XLoader, be, malware]\"} ```\n",
    "        \n",
    "        Rule 5: If the [subject,relation] in a triple can be formed into a new [subject,relation,object] triple because relation itself has a new object in it, create a new triple while keeping the original one. \n",
    "        Example: Input:\"[Formbook, save XLoader to, desktop]\". Output:```json {\"Thoughts\": \"The relation \"save XLoader to\" can be split into a new triple.\", \"Triple1\": \"[Formbook, save, XLoader]\", \"Triple2\": \"[Formbook, save XLoader to, desktop]\"} ```\n",
    "        \n",
    "        Rule 6: If Subject or Object contains an MD5, registry, path, or other identifier that contains prefixes, remove their prefixes to generate a new triple.\n",
    "        Example for Rule 6: Input: \"[Formbook's hash, is, md5:xxxxx]\". Output:```json {\"Thoughts\": \"The object contains an MD5 identifier, so I removed the prefix.\", \"Triple\": \"[Formbook's hash, is, xxxxx]\"} ```\n",
    "        \n",
    "       Rule 7: If  Subject or Object contain version information or OS information or CVE number or other specific information, keep them and apply Rule 1-6 to the rest of the triple.\n",
    "        Example for Rule 7: Input: \"[Formbook v1.0, is, malware]\". Output:```json {\"Thoughts\": \"The subject contains version information, so I kept it and applied Rule 1-6 to the rest of the triple.\", \"Triple\": \"[Formbook v1.0, be, malware]\"} ```\n",
    "        \n",
    "        Output format requirement:\n",
    "        Your output MUST be in JSON format. The keys are \\\"Thoughts\\\" and \\\"Triple\\\". The value of \\\"Thoughts\\\" is a string describing your thought process for modifying the triple, and the value of \\\"Triple\\\" is the modified triple. Don't use any other format, if my input has nothing, you don't need to write anything, only output this JSON:\n",
    "        ```json\n",
    "        [\n",
    "        {\n",
    "            \"Thoughts\": \"No input\",\n",
    "            \"Triple\": \"\", \n",
    "        ]```\n",
    "        \n",
    "        A example of the output format is shown below:\n",
    "        ```json\n",
    "        [\n",
    "        {\n",
    "            \"Thoughts\": \"Example of thoughts\",\n",
    "            \"Triple1\": \"Example of triple1 because the original triple can split into multiple triples.\", \n",
    "            \"Triple2\": \"Example of triple1 because the original triple can split into multiple triples.\"\n",
    "        },\n",
    "        {\n",
    "            \"Thoughts\": \"SExample of thoughts\",\n",
    "            \"Triple\": \"Example of triple after the modification based on rules.\"\n",
    "        }\n",
    "    ]```\n",
    "        '''\n",
    "        \"Here is my entity extraction result:\\\"\"+str(text)+\"\"\"\\\". Now, you apply the rules above to modify the triples. \n",
    "        \n",
    "        \"\"\"\n",
    "        },\n",
    "        ]\n",
    "        return promptmessage\n",
    "\n",
    "all_prompts_Refiner=[]\n",
    "for index in range(Core_DataFrame.shape[0]):\n",
    "    text_thispart=Core_DataFrame.loc[index, 'Integrator_result']\n",
    "    \n",
    "    promptmessage=generate_prompt_postprocess(text_thispart)\n",
    "    \n",
    "    all_prompts_Refiner.append(promptmessage)\n",
    "\n",
    "code_name=\"SECCG Refiner agent\"\n",
    "jsonl_file= tools.create_jsonl(all_prompts_Refiner,model='gpt4', temp=1, token=16384, jsonlname=code_name, possible_output=300)\n",
    "\n",
    "ids = tools.upload_RUN_PAY_jsonl(jsonl_file, code_name)\n",
    "\n",
    "ans=tools.auto_down_ans(ids['batch_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b1f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tools.ans_to_df(ans)\n",
    "\n",
    "def capture_triples(text):\n",
    "    triples = re.findall(r'\"Triple\\d*\":\\s*\"(\\[.*?\\])\"', text)\n",
    "    return triples\n",
    "\n",
    "triples_str=[capture_triples(i) for i in ans]\n",
    "\n",
    "Core_DataFrame['Refiner_result']=triples_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac035974",
   "metadata": {},
   "source": [
    "Merger agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd76d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(longmem,shortmem):\n",
    "        promptmessage = [\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \n",
    "        '''You are a triples integration assistant. Triple is a basic data structure, which describes concepts and their relationships. A triple in long-term and short-term memory MUST has THREE elements: [Subject, Relation, Object]. You are now reading a whole article and extract all triples from it. But you can only see part of the article at a time. In order to record all the triples from a article, you have the following long-term memory area to record the triples from the entire article. long-term memory stores information on the aricle parts you have already read.\n",
    "        -The start of the long-term memory area-\n",
    "        #Triples will be added here\n",
    "        -The end of the short-term memory area-\n",
    "        Second, you now see a part of this article. Based on this part, you already extract such triples and place them in your short-term memory: \n",
    "        -The start of the short-term memory area-\n",
    "        #Triples will be added here\n",
    "       -The end of the short-term memory area-\n",
    "        Third, now review your long-term memory and short-term memory. Modify the short-term memory into a new short-term memory. You should follow following rules to modify triples in short-term memory to make them consistent with triples in long-term memory. You should write down how you use the rule to modify the triples in short-term memory. \n",
    "        \n",
    "        Rule 1. You notice that in these triples, some triples have subjects and objects that contain partially identical terms and refer to the same specific nouns, but these specific nouns have prefixes/suffixes/modifiers that make them not identical. You should delete the prefixes/suffixes/modifiers and unify them into the same specific nouns.\n",
    "        \n",
    "        Before rule: [the Formbook, is designed to run as, a deleter] [Formbook sample, is designed to run as, one-time encryptor]\n",
    "\n",
    "        After rule: [Formbook, is designed to run as, a deleter] [Formbook, is designed to run as, one-time encryptor]\n",
    "\n",
    "        Explanation: The words \"the Formbook\" and \"Formbook sample\" refer to the same entity, so they are unified to use the exact same subject \"Formbook\" for consistency.\n",
    "        \n",
    "        Rule 2. Be especially careful that when you meet specific names of malware,CVE, Trojans, hacker organizations, etc., always use their specific names and remove the prefixes/suffixes/modifiers.\n",
    "        \n",
    "        Before rule: [Malware Formbook, is, malware] \n",
    "        \n",
    "        After rule: [Formbook, is, malware]\n",
    "        \n",
    "        Explanation: The word \"Formbook\" is a specific name of malware, so it should be used as the subject of the triple and the prefix \"Malware\" should be removed.\n",
    "        \n",
    "        Rule 3. Don't add unexisting triples to your new short-term memory. \n",
    "    \n",
    "        Suppose you find in long-term memory: [the malware, download, Leafminer] and in short-term memory: [Formbook, is, malware]. You cannot add a new triple in new short term memory: [Formbook, download, Leafminer]. Because you don't have evidence that \"the malware\" in the long-term memory specifically refers to \"Formbook\".\n",
    "        \n",
    "        Rule 4. Don't add unexisting triples that don't exsit in long-term memory or short-term memory to your new short-term memory. You should add triples from long-term memory or short-term memory to your new short-term memory, not from your imagination and selfcreation\n",
    "        \n",
    "        Rule 5. Don't add any example word like 'Formbook','XLoader','Leafminer', 'FinSpy', 'Kismet' in your new short-term memory area, they are just example words not the real triples in the long term memory area or short term memory area.\n",
    "        \n",
    "        Rule 6. new short-term memory area must be started with \\'-The start of new short-term memory area-\\' and ended with \\'-The end of new short-term memory area-\\'. A triple in new short-term memory MUST has THREE elements: [Subject, Relation, Object]. \n",
    "        '''\n",
    "        },\n",
    "        {\"role\": \"assistant\",'content': 'Yes, I understand and totally follow the rules.'},\n",
    "        {\"role\": \"user\",'content': '''\n",
    "         -The start of the long-term memory area-  \n",
    "         [SUBJECT:AgumonHacker, RELATION:disrupts, OBJECT:network operations], [SUBJECT:HackGroup Gabumon, RELATION:targets, OBJECT:financial institutions], [SUBJECT:Malware Biyomon, RELATION:steals, OBJECT:personal data], [SUBJECT:CyberAttack 2042, RELATION:compromised, OBJECT:database systems],  [SUBJECT:A sample of Biyomon, RELATION:save, OBJECT:a payload]\n",
    "         -The end of the long-term memory area-\n",
    "\n",
    "        -The start of the short-term memory area-  \n",
    "         [SUBJECT:The AgumonHacker Trojan, RELATION:created by, OBJECT:a sophisticated hacker group], [SUBJECT:Gabumon hackers, RELATION:launched, OBJECT:series of phishing campaigns], [SUBJECT:Biyomon malware program, RELATION:encrypts, OBJECT:files for ransom], [SUBJECT:Attack in 2042, RELATION:led to, OBJECT:sensitive leaks],[SUBJECT:Biyomon, RELATION:run, OBJECT:payload]\n",
    "        -The end of the short-term memory area-\n",
    "         '''\n",
    "         },\n",
    "        {\"role\": \"assistant\",'content': \n",
    "         '''\n",
    "         Modifications required based on the rules:\n",
    "        Apply Rule 1 and Rule 2: Unify subjects and objects with partially identical terms and remove any prefixes/suffixes/modifiers, particularly for specific names like malware and hacker groups.\n",
    "        Modified short-term memory:\n",
    "\n",
    "        [SUBJECT:The AgumonHacker Trojan, RELATION:created by, OBJECT:a sophisticated hacker group] → [SUBJECT:AgumonHacker, RELATION:created by, OBJECT:sophisticated hacker group]\n",
    "        [SUBJECT:Gabumon hackers, RELATION:launched, OBJECT:series of phishing campaigns] → [SUBJECT:HackGroup Gabumon, RELATION:launched, OBJECT:phishing campaigns]\n",
    "        [SUBJECT:Biyomon malware program, RELATION:encrypts, OBJECT:files for ransom] → [SUBJECT:Malware Biyomon, RELATION:encrypts, OBJECT:files for ransom]\n",
    "        [SUBJECT:Attack in 2042, RELATION:led to, OBJECT:sensitive leaks] → [SUBJECT:CyberAttack 2042, RELATION:led to, OBJECT:sensitive data leaks]\n",
    "\n",
    "        I also find a triple in long-term memory that needs to be modified based on Rule 1 and Rule 2:\n",
    "\n",
    "        [SUBJECT:A sample of Biyomon, RELATION:save, OBJECT:a payload] → [SUBJECT:Biyomon, RELATION:save, OBJECT:payload]\n",
    "\n",
    "        Apply Rule 3 and 4 and 5: We must ensure we do not create unexisting triples when merging the memories.\n",
    "        After applying the modifications, we now create a new short-term memory area:\n",
    "\n",
    "        -The start of new short-term memory area- \n",
    "        [SUBJECT:AgumonHacker, RELATION:created by, OBJECT:sophisticated hacker group], \n",
    "        [SUBJECT:HackGroup Gabumon, RELATION:launched, OBJECT:phishing campaigns],\n",
    "        [SUBJECT:Biyomon, RELATION:encrypts, OBJECT:files for ransom],\n",
    "        [SUBJECT:CyberAttack 2042, RELATION:led to, OBJECT:sensitive data leaks],\n",
    "        [SUBJECT:Biyomon, RELATION:run, OBJECT:payload],\n",
    "        [SUBJECT:Biyomon, RELATION:save, OBJECT:payload] \n",
    "        -The end of new short-term memory area-\n",
    "         '''   },\n",
    "        {\"role\": \"user\",'content': \n",
    "        '''\n",
    "        Good. Now, let's swtich to another article. \n",
    "        -The start of the long-term memory area-\n",
    "        '''+str(longmem)+'''\n",
    "        -The end of the long-term memory area-\n",
    "    \n",
    "        -The start of the short-term memory area-\n",
    "        '''+str(shortmem)+'''\n",
    "        -The end of the short-term memory area-\n",
    "        \n",
    "        Now, follow the rules. Write down how you use the rule to modify the triples in short-term memory. If there is no any triple in my input short-term memory, you still need to write down \\'-The start of new short-term memory area-\\' and ended with \\'-The end of new short-term memory area-\\'. with only one blank line between them. If my input short-term memory is already perfect, you still need to write down \\'-The start of new short-term memory area-\\' and content of that perfect short-term memory and ended with \\'-The end of new short-term memory area-\\'.\n",
    "        '''\n",
    "        },      \n",
    "        ]\n",
    "        return promptmessage\n",
    "\n",
    "def check_brackets(my_string):\n",
    "    if my_string is None or len(my_string) == 0:\n",
    "        return False\n",
    "    my_string = my_string.strip()\n",
    "    first_char_is_bracket = my_string[0] == '['\n",
    "    last_char_is_bracket = my_string[-1] == ']'\n",
    "\n",
    "    if first_char_is_bracket and last_char_is_bracket:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "  \n",
    "def checker(my_string):\n",
    "    promptmessage = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\":'You are a result checker. You are responsible for checking the result from other AI assistants. The AI assistant may say that \\\" I am sorry, but I am Chat AI model and I am not able to do the task \\\" or \\\" You should do it by yourself\\\" or \\\"I am sorry, but I am not able to do the task\\\". If you found those words or words with simlar meaning, you must reply me \\\"ERROR\\\", other wise, you should reply me \\\"OK\\\". Here is the result from other AI assistant: '+str(my_string)}]\n",
    "    import os\n",
    "    from openai import OpenAI\n",
    "    setmodel='<GPTNAME>'\n",
    "    api_key = \"<APIKEY>\"\n",
    "    api_base = \"<APIBASE>\"\n",
    "\n",
    "    client = OpenAI(api_key=api_key, base_url=api_base)\n",
    "    stream = client.chat.completions.create(\n",
    "        model=setmodel,\n",
    "        messages=promptmessage,\n",
    "        stream=True,\n",
    "        max_tokens=128,\n",
    "        temperature=1,\n",
    "    )\n",
    "    final_response = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            #print(chunk.choices[0].delta.content, end=\"\")\n",
    "            final_response += chunk.choices[0].delta.content  \n",
    "    return final_response \n",
    "\n",
    "grouped = Core_DataFrame.groupby('content')\n",
    "\n",
    "list_of_dfs = [group for _, group in grouped]\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pickle\n",
    "\n",
    "# Assuming merge_extracted_triples(), check_brackets(), and checker() are already defined\n",
    "\n",
    "triple_cache = []\n",
    "text_cache = []\n",
    "longmem = None\n",
    "\n",
    "# Iterate over each sub DataFrame\n",
    "for sub_df in list_of_dfs:\n",
    "    # Iterate through each row in the sub DataFrame\n",
    "    for i, row in sub_df.iterrows():\n",
    "        clean_triple_forMEM = row['content part']  # Assuming this is the column for content parts\n",
    "        this_time_test = row['some_other_column']  # Replace with actual column name\n",
    "\n",
    "        if i == 0:\n",
    "            if check_brackets(clean_triple_forMEM):\n",
    "                longmem = clean_triple_forMEM\n",
    "            else:\n",
    "                longmem = 'No longterm memory'\n",
    "            triple_cache.append(clean_triple_forMEM)\n",
    "            text_cache.append(this_time_test)\n",
    "            print('First thinking completed')\n",
    "\n",
    "        if i >= 1:\n",
    "            print('Past long-term memory is:')\n",
    "            print(longmem)\n",
    "            original_longmem = longmem\n",
    "            if len(longmem) >= 1500:\n",
    "                longmem = longmem[-1000:]\n",
    "                if '[' in longmem:\n",
    "                    longmem = longmem[longmem.index('['):]\n",
    "\n",
    "            if check_brackets(clean_triple_forMEM):\n",
    "                max_retries = 3\n",
    "                retry_count = 0\n",
    "                while retry_count < max_retries:\n",
    "                    print('Retry ' + str(retry_count) + ' times')\n",
    "                    newlongmem = generate_prompt(longmem, clean_triple_forMEM, this_time_test)\n",
    "                    print('Thinking process:')\n",
    "                    print(newlongmem)\n",
    "                    newlongmem = newlongmem.replace('-The start of the new short-term memory area-', '-The start of new short-term memory area-')\n",
    "                    newlongmem = newlongmem.replace('-The end of the new short-term memory area-', '-The end of new short-term memory area-')\n",
    "\n",
    "                    if '-The start of new short-term memory area-' in newlongmem and '-The end of new short-term memory area-' in newlongmem and checker(newlongmem) != 'ERROR':\n",
    "                        newlongmem = newlongmem[newlongmem.rindex('-The start of new short-term memory area-') + len('-The start of new short-term memory area-'):newlongmem.rindex('-The end of new short-term memory area-')]\n",
    "                        if not any(keyword in newlongmem for keyword in ['Formbook', 'XLoader', 'savetextfile', 'Leafminer', 'FinSpy', 'Kismet', 'Agumon', 'Gabumon', 'Biyomon', '2042']):\n",
    "                            longmem = str(original_longmem) + ', ' + str(newlongmem)\n",
    "                            retry_count = 9999\n",
    "                        else:\n",
    "                            retry_count += 1\n",
    "                    else:\n",
    "                        retry_count += 1\n",
    "            else:\n",
    "                longmem = original_longmem\n",
    "                print('Short-term memory is not a triple')\n",
    "            print('After merging: The new long-term memory is:')\n",
    "            print(longmem)\n",
    "\n",
    "            # Create a new DataFrame for saving\n",
    "            new_data = pd.DataFrame({'single_article': [str(row['content'])], 'longmem': [str(longmem)]})\n",
    "\n",
    "            try:\n",
    "                # Read the existing Excel file\n",
    "                longmem_cache = pd.read_excel('Knowledge Graph result cache backup.xlsx')\n",
    "                # Add new data to the end of existing data\n",
    "                longmem_cache = pd.concat([longmem_cache, new_data], ignore_index=True)\n",
    "            except FileNotFoundError:\n",
    "                # If the file does not exist, use the new data directly\n",
    "                longmem_cache = new_data\n",
    "\n",
    "            # Save the updated data to the Excel file\n",
    "            longmem_cache.to_excel('Knowledge Graph result cache backup.xlsx', index=False)\n",
    "            # Add the result to the cache"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
