{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "# Read nvd_cve_df_sample4x25 from the pickle file\n",
    "with open('random_sampled_100CVEs_names.pkl', 'rb') as file:\n",
    "    nvd_cve_df_sample4x25 = pickle.load(file)\n",
    "\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "def search_urls(query, mode='D', max_results=20):\n",
    "    try:\n",
    "        if mode == 'D':\n",
    "            results = DDGS().text(query,safesearch='off', max_results=max_results)\n",
    "            urls = [i['href'] for i in results if 'href' in i]\n",
    "            urls={\"only_url\":urls,'full_result':results}\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Use 'D' for DuckDuckGo \")\n",
    "        return urls\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error searching URLs: {e}')\n",
    "        return []\n",
    "    \n",
    "import pandas as pd\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "def download_urls(urls, driver_path='/usr/bin/chromedriver', render_wait_time=20, filename='downloaded_urls'):\n",
    "    temp_file = f'df_download_urls_temp_{filename}.xlsx'\n",
    "    final_file = f'df_download_urls_{filename}.xlsx'\n",
    "    df = pd.DataFrame(urls, columns=['url'])\n",
    "    df['content'] = None\n",
    "    df['attempt'] = 0\n",
    "\n",
    "    def job():\n",
    "        if df[(df['attempt'] <= 3) & (df['content'].isnull())].empty:\n",
    "            print(\"All URLs with attempt <= 3 have been processed, stopping the task.\")\n",
    "            return\n",
    "        \n",
    "        count = len(df[(df['content'].isnull()) & (df['attempt'] <= 3)])\n",
    "        print(f\"There are currently {count} URLs pending processing.\")\n",
    "        \n",
    "        sample_urls = df[(df['content'].isnull()) & (df['attempt'] <= 3)].sample(min(18, len(df[(df['content'].isnull()) & (df['attempt'] <= 3)])))\n",
    "        df.loc[df['url'].isin(sample_urls['url']), 'attempt'] += 1\n",
    "        print(\"Selected URLs for this batch:\", sample_urls['url'].tolist())\n",
    "        \n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            future_to_url = {executor.submit(fetch_url_content_with_selenium_and_jinareader, url, driver_path, render_wait_time): url for url in sample_urls['url']}\n",
    "            for future in as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    url, content = future.result()\n",
    "                    if \"failed fetch_url_content_with_selenium_and_jinareader\" not in content:\n",
    "                        df.loc[df['url'] == url, 'content'] = content\n",
    "                except Exception as e:\n",
    "                    print(f\"Error occurred while processing {url}: {e}\")\n",
    "        \n",
    "    start_time = time.time()\n",
    "    start_seconds = (start_time + 3) % 60\n",
    "    \n",
    "    print(\"Start time:\", time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time)))\n",
    "    while True:\n",
    "        if df[(df['attempt'] <= 3) & (df['content'].isnull())].empty:\n",
    "            print(\"All URLs with attempt <= 3 have been processed, stopping the task.\")\n",
    "            break \n",
    "        if int(start_seconds) == int(time.time()) % 60:\n",
    "            print(\"Starting this batch of work.\")\n",
    "            job()\n",
    "            df.to_excel(temp_file, index=False)\n",
    "            print(\"Number of URLs processed so far:\", len(df[df['content'].notnull()]))\n",
    "            print(\"Saving intermediate file.\")\n",
    "    df.to_excel(final_file, index=False)\n",
    "    print(\"Saving final file.\")   \n",
    "    return df\n",
    "\n",
    "def fetch_url_content_with_selenium_and_jinareader(source_url, driver_path, render_wait_time):\n",
    "    full_url = \"https://r.jina.ai/\" + source_url\n",
    "    print(\"fetch_url_content_with_selenium_and_jinareader starting work on:\", full_url)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--disable-extensions')\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    service = Service(driver_path)\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    try:\n",
    "        driver.get(full_url)\n",
    "        WebDriverWait(driver, render_wait_time).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"html\"))\n",
    "        )\n",
    "        time.sleep(3)\n",
    "        content = driver.page_source\n",
    "        \n",
    "        if 'blocked until' and 'due to previous abuse found on' in content:\n",
    "            print(\"Alert! Domain has been blocked. Content:\", content)\n",
    "            return source_url, 'failed fetch_url_content_with_selenium_and_jinareader get blocked'\n",
    "        if \"Per IP rate limit exceeded\" in content:\n",
    "            print(\"Alert! IP rate limit exceeded. Content:\", content)\n",
    "            return source_url, 'failed fetch_url_content_with_selenium_and_jinareader IP rate limit exceeded'\n",
    "        if 'Slow down, turbo' in content:\n",
    "            print(\"Alert! Speed too fast. Content:\", content)\n",
    "            return source_url, 'failed fetch_url_content_with_selenium_and_jinareader speed too fast'\n",
    "        if '</body></html>' not in content:\n",
    "            print(\"Alert! Content not complete. Content:\", content)\n",
    "            return source_url, 'failed fetch_url_content_with_selenium_and_jinareader content not complete'\n",
    "        print(\"fetch_url_content_with_selenium_and_jinareader successfully fetched content.\")\n",
    "        return source_url, content\n",
    "    \n",
    "    except TimeoutException as te:\n",
    "        print(\"fetch_url_content_with_selenium_and_jinareader timed out:\", te)\n",
    "        return source_url, 'failed fetch_url_content_with_selenium_and_jinareader time out'\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"fetch_url_content_with_selenium_and_jinareader encountered an error:\", e)\n",
    "        return source_url, 'failed fetch_url_content_with_selenium_and_jinareader unknown error'\n",
    "    \n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        print(\"fetch_url_content_with_selenium_and_jinareader runtime:\", end_time - start_time)\n",
    "        driver.quit()\n",
    "\n",
    "import sys, pickle, os, json, re, time, random, logging, pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns, scipy, sklearn, networkx as nx, importlib;\n",
    "\n",
    "import pandas as pd  # type: ignore\n",
    "import tools\n",
    "import time\n",
    "import signal\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def handler(signum, frame):\n",
    "    raise TimeoutException()\n",
    "\n",
    "signal.signal(signal.SIGALRM, handler)\n",
    "\n",
    "def domain_set_count(urls, blacklist=None):\n",
    "    from urllib.parse import urlparse\n",
    "    return len(set(urlparse(url).netloc for url in urls))\n",
    "\n",
    "def domain_count(urls, blacklist=None):\n",
    "    from urllib.parse import urlparse\n",
    "    return len([urlparse(url).netloc for url in urls])\n",
    "\n",
    "cve_to_crawl = nvd_cve_df_sample4x25.CVE.tolist()\n",
    "cve_to_crawl = cve_to_crawl[0:3]\n",
    "\n",
    "cve_search_results = {}\n",
    "cve_search_content = {}\n",
    "for single_cve in cve_to_crawl:\n",
    "    try:\n",
    "        print('working on', single_cve)\n",
    "        signal.alarm(300)\n",
    "        search_results = search_urls(str(\"\\\"\" + single_cve + \"\\\"\"), max_results=40)\n",
    "        cve_content = {\n",
    "            \"full name\": single_cve,\n",
    "            \"urls result\": search_results['only_url'],\n",
    "            \"full search results\": search_results['full_result'],\n",
    "            \"domain count\": domain_count(search_results['only_url']),\n",
    "            \"domain set count\": domain_set_count(search_results['only_url'])\n",
    "        }\n",
    "        with open(\"crawler_realtime_data.txt\", \"a\") as file:\n",
    "            file.write(f\"Current time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))}\\n\")\n",
    "            file.write(f\"Completed search for {single_cve}, found {len(search_results['only_url'])} URLs, \"\n",
    "                       f\"with {domain_count(search_results['only_url'])} unique domains, and {domain_set_count(search_results['only_url'])} unique domain sets\\n\")\n",
    "            file.write(f\"Search result URLs: {search_results['only_url']}\\n\")\n",
    "        cve_search_results[single_cve] = cve_content\n",
    "\n",
    "        # Start crawling\n",
    "        df_content = download_urls(search_results['only_url'], filename=single_cve)\n",
    "\n",
    "        cve_search_content[single_cve] = df_content\n",
    "        with open(\"crawler_realtime_data.txt\", \"a\") as file:\n",
    "            file.write(f\"Completed downloading for {single_cve}, downloaded {len(df_content)} files\\n\")\n",
    "        print(f'End time: {time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(time.time()))}')\n",
    "\n",
    "        # Cancel the timer\n",
    "        signal.alarm(0)\n",
    "\n",
    "        # Wait for 10 seconds\n",
    "        time.sleep(10)\n",
    "\n",
    "    except TimeoutException:\n",
    "        with open(\"error_elements.txt\", \"a\") as error_file:\n",
    "            error_file.write(f\"{single_cve}\\n\")\n",
    "        print(f\"{single_cve} exceeded time limit, skipping.\")\n",
    "        signal.alarm(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
